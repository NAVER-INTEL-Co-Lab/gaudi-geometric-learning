{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tFC-bQ87Amb"
   },
   "source": [
    "# Install Pytorch and PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zWn5yzT0LOzH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_HPU_LAZY_MODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0a0+git74cd574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the eager mode\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '0'\n",
    "\n",
    "# Verify the environment variable is set\n",
    "print(f\"PT_HPU_LAZY_MODE: {os.environ['PT_HPU_LAZY_MODE']}\")\n",
    "\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import habana_frameworks.torch.core as htcore\n",
    "\n",
    "# use rich traceback\n",
    "\n",
    "from rich import traceback\n",
    "traceback.install()\n",
    "\n",
    "device = torch.device(\"hpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDy46FIQ6OWN"
   },
   "source": [
    "# Customizing Aggregations within Message Passing\n",
    "\n",
    "Aggregation functions play an important role in the message passing framework and the readout function when implementing GNNs. Many works in the GNN literature ([Hamilton et al. (2017)](https://cs.stanford.edu/~jure/pubs/graphsage-nips17.pdf), [Xu et al. (2018)](https://arxiv.org/abs/1810.00826), [Corso et al. (2020)](https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf), [Li et al. (2020)](https://arxiv.org/abs/2006.07739)), demonstrate that the choice of aggregation functions contributes significantly to the performance of GNN models. In particular, the performance of GNNs with different aggregation functions differs when applied to distinct tasks and datasets. Recent works also show that using multiple aggregations ([Corso et al. (2020)](https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf)) and learnable aggregations ([Li et al. (2020)](https://arxiv.org/abs/2006.07739)) can potentially gain substantial improvements. To facilitate experimentation with these different aggregation schemes and unify concepts of aggregation within GNNs across both [`MessagePassing`](https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py) and [global readouts](https://github.com/pyg-team/pytorch_geometric/tree/master/torch_geometric/nn/glob), we provide **modular and re-usable aggregations** in the newly defined `torch_geometric.nn.aggr.*` package. Unifying these concepts also helps us to perform optimization and specialized implementations in a single place. In the new integration, the following functionality is applicable:\n",
    "\n",
    "```python\n",
    "# Original interface with string type as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=\"mean\")\n",
    "\n",
    "\n",
    "# Use a single aggregation module as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=MeanAggregation())\n",
    "\n",
    "\n",
    "# Use a list of aggregation strings as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=['mean', 'max', 'sum', 'std', 'var'])\n",
    "\n",
    "\n",
    "# Use a list of aggregation modules as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=[\n",
    "            MeanAggregation(),\n",
    "            MaxAggregation(),\n",
    "            SumAggregation(),\n",
    "            StdAggregation(),\n",
    "            VarAggregation(),\n",
    "        ])\n",
    "\n",
    "\n",
    "# Use a list of mixed modules and strings as aggregation argument\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=[\n",
    "            'mean',\n",
    "            MaxAggregation(),\n",
    "            'sum',\n",
    "            StdAggregation(),\n",
    "            'var',\n",
    "        ])\n",
    "\n",
    "\n",
    "# Define multiple aggregations with `MultiAggregation` module\n",
    "class MyConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        super().__init__(aggr=MultiAggregation([\n",
    "            SoftmaxAggregation(t=0.1, learn=True),\n",
    "            SoftmaxAggregation(t=1, learn=True),\n",
    "            SoftmaxAggregation(t=10, learn=True)\n",
    "        ]))\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3W4IfAWgSXVy"
   },
   "source": [
    "In this tutorial, we explore the new aggregation package with `SAGEConv` ([Hamilton et al. (2017)](https://cs.stanford.edu/~jure/pubs/graphsage-nips17.pdf)) and `ClusterLoader` ([Chiang et al. (2019)](https://arxiv.org/abs/1905.07953)) and showcase on the `PubMed` graph from the `Planetoid` node classification benchmark suite ([Yang et al. (2016)](https://arxiv.org/abs/1603.08861))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_s25N9oyBIy"
   },
   "source": [
    "## Loading the dataset\n",
    "Let's first load the `Planetoid` dataset and create subgraphs with `ClusterData` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ops.load_library(\"/root/raw_torch_for_scatter/metis/csrc/build/libtsmetis.so\")\n",
    "torch.ops.load_library(\"/root/raw_torch_for_scatter/pytorch_cluster/csrc/build/librandom_walk.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_geometric/loader/cluster.py\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import Tensor\n",
    "\n",
    "import torch_geometric.typing\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.index import index2ptr, ptr2index\n",
    "from torch_geometric.io import fs\n",
    "from torch_geometric.typing import pyg_lib\n",
    "from torch_geometric.utils import index_sort, narrow, select, sort_edge_index\n",
    "from torch_geometric.utils.map import map_index\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Partition:\n",
    "    indptr: Tensor\n",
    "    index: Tensor\n",
    "    partptr: Tensor\n",
    "    node_perm: Tensor\n",
    "    edge_perm: Tensor\n",
    "    sparse_format: Literal['csr', 'csc']\n",
    "\n",
    "\n",
    "class ClusterData(torch.utils.data.Dataset):\n",
    "    r\"\"\"Clusters/partitions a graph data object into multiple subgraphs, as\n",
    "    motivated by the `\"Cluster-GCN: An Efficient Algorithm for Training Deep\n",
    "    and Large Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1905.07953>`_ paper.\n",
    "\n",
    "    .. note::\n",
    "        The underlying METIS algorithm requires undirected graphs as input.\n",
    "\n",
    "    Args:\n",
    "        data (torch_geometric.data.Data): The graph data object.\n",
    "        num_parts (int): The number of partitions.\n",
    "        recursive (bool, optional): If set to :obj:`True`, will use multilevel\n",
    "            recursive bisection instead of multilevel k-way partitioning.\n",
    "            (default: :obj:`False`)\n",
    "        save_dir (str, optional): If set, will save the partitioned data to the\n",
    "            :obj:`save_dir` directory for faster re-use. (default: :obj:`None`)\n",
    "        filename (str, optional): Name of the stored partitioned file.\n",
    "            (default: :obj:`None`)\n",
    "        log (bool, optional): If set to :obj:`False`, will not log any\n",
    "            progress. (default: :obj:`True`)\n",
    "        keep_inter_cluster_edges (bool, optional): If set to :obj:`True`,\n",
    "            will keep inter-cluster edge connections. (default: :obj:`False`)\n",
    "        sparse_format (str, optional): The sparse format to use for computing\n",
    "            partitions. (default: :obj:`\"csr\"`)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data,\n",
    "        num_parts: int,\n",
    "        recursive: bool = False,\n",
    "        save_dir: Optional[str] = None,\n",
    "        filename: Optional[str] = None,\n",
    "        log: bool = True,\n",
    "        keep_inter_cluster_edges: bool = False,\n",
    "        sparse_format: Literal['csr', 'csc'] = 'csr',\n",
    "    ):\n",
    "        assert data.edge_index is not None\n",
    "        assert sparse_format in ['csr', 'csc']\n",
    "\n",
    "        self.num_parts = num_parts\n",
    "        self.recursive = recursive\n",
    "        self.keep_inter_cluster_edges = keep_inter_cluster_edges\n",
    "        self.sparse_format = sparse_format\n",
    "\n",
    "        recursive_str = '_recursive' if recursive else ''\n",
    "        root_dir = osp.join(save_dir or '', f'part_{num_parts}{recursive_str}')\n",
    "        path = osp.join(root_dir, filename or 'metis.pt')\n",
    "\n",
    "        if save_dir is not None and osp.exists(path):\n",
    "            self.partition = fs.torch_load(path)\n",
    "        else:\n",
    "            if log:  # pragma: no cover\n",
    "                print('Computing METIS partitioning...', file=sys.stderr)\n",
    "\n",
    "            cluster = self._metis(data.edge_index, data.num_nodes)\n",
    "            self.partition = self._partition(data.edge_index, cluster)\n",
    "\n",
    "            if save_dir is not None:\n",
    "                os.makedirs(root_dir, exist_ok=True)\n",
    "                torch.save(self.partition, path)\n",
    "\n",
    "            if log:  # pragma: no cover\n",
    "                print('Done!', file=sys.stderr)\n",
    "\n",
    "        self.data = self._permute_data(data, self.partition)\n",
    "\n",
    "    def _metis(self, edge_index: Tensor, num_nodes: int) -> Tensor:\n",
    "        # Computes a node-level partition assignment vector via METIS.\n",
    "        if self.sparse_format == 'csr':  # Calculate CSR representation:\n",
    "            row, index = sort_edge_index(edge_index, num_nodes=num_nodes)\n",
    "            indptr = index2ptr(row, size=num_nodes)\n",
    "        else:  # Calculate CSC representation:\n",
    "            index, col = sort_edge_index(edge_index, num_nodes=num_nodes,\n",
    "                                         sort_by_row=False)\n",
    "            indptr = index2ptr(col, size=num_nodes)\n",
    "\n",
    "        # Compute METIS partitioning:\n",
    "        cluster: Optional[Tensor] = None\n",
    "        \n",
    "        cluster = torch.ops.torch_sparse.partition(\n",
    "            indptr.cpu(),\n",
    "            index.cpu(),\n",
    "            None,\n",
    "            self.num_parts,\n",
    "            self.recursive,\n",
    "        ).to(edge_index.device)\n",
    "\n",
    "        return cluster\n",
    "\n",
    "    def _partition(self, edge_index: Tensor, cluster: Tensor) -> Partition:\n",
    "        # Computes node-level and edge-level permutations and permutes the edge\n",
    "        # connectivity accordingly:\n",
    "\n",
    "        # Sort `cluster` and compute boundaries `partptr`:\n",
    "        cluster, node_perm = index_sort(cluster, max_value=self.num_parts)\n",
    "        partptr = index2ptr(cluster, size=self.num_parts)\n",
    "\n",
    "        # Permute `edge_index` based on node permutation:\n",
    "        edge_perm = torch.arange(edge_index.size(1), device=edge_index.device)\n",
    "        arange = torch.empty_like(node_perm)\n",
    "        arange[node_perm] = torch.arange(cluster.numel(),\n",
    "                                         device=cluster.device)\n",
    "        edge_index = arange[edge_index]\n",
    "\n",
    "        # Compute final CSR representation:\n",
    "        (row, col), edge_perm = sort_edge_index(\n",
    "            edge_index,\n",
    "            edge_attr=edge_perm,\n",
    "            num_nodes=cluster.numel(),\n",
    "            sort_by_row=self.sparse_format == 'csr',\n",
    "        )\n",
    "        if self.sparse_format == 'csr':\n",
    "            indptr, index = index2ptr(row, size=cluster.numel()), col\n",
    "        else:\n",
    "            indptr, index = index2ptr(col, size=cluster.numel()), row\n",
    "\n",
    "        return Partition(indptr, index, partptr, node_perm, edge_perm,\n",
    "                         self.sparse_format)\n",
    "\n",
    "    def _permute_data(self, data: Data, partition: Partition) -> Data:\n",
    "        # Permute node-level and edge-level attributes according to the\n",
    "        # calculated permutations in `Partition`:\n",
    "        out = copy.copy(data)\n",
    "        for key, value in data.items():\n",
    "            if key == 'edge_index':\n",
    "                continue\n",
    "            elif data.is_node_attr(key):\n",
    "                cat_dim = data.__cat_dim__(key, value)\n",
    "                out[key] = select(value, partition.node_perm, dim=cat_dim)\n",
    "            elif data.is_edge_attr(key):\n",
    "                cat_dim = data.__cat_dim__(key, value)\n",
    "                value = value.tolist()                \n",
    "                out[key] = select(value, partition.edge_perm, dim=cat_dim)\n",
    "        out.edge_index = None\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.partition.partptr.numel() - 1\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Data:\n",
    "        node_start = int(self.partition.partptr[idx])\n",
    "        node_end = int(self.partition.partptr[idx + 1])\n",
    "        node_length = node_end - node_start\n",
    "\n",
    "        indptr = self.partition.indptr[node_start:node_end + 1]\n",
    "        edge_start = int(indptr[0])\n",
    "        edge_end = int(indptr[-1])\n",
    "        edge_length = edge_end - edge_start\n",
    "        indptr = indptr - edge_start\n",
    "\n",
    "        if self.sparse_format == 'csr':\n",
    "            row = ptr2index(indptr)\n",
    "            col = self.partition.index[edge_start:edge_end]\n",
    "            if not self.keep_inter_cluster_edges:\n",
    "                edge_mask = (col >= node_start) & (col < node_end)\n",
    "                row = row[edge_mask]\n",
    "                col = col[edge_mask] - node_start\n",
    "        else:\n",
    "            col = ptr2index(indptr)\n",
    "            row = self.partition.index[edge_start:edge_end]\n",
    "            if not self.keep_inter_cluster_edges:\n",
    "                edge_mask = (row >= node_start) & (row < node_end)\n",
    "                col = col[edge_mask]\n",
    "                row = row[edge_mask] - node_start\n",
    "\n",
    "        out = copy.copy(self.data)\n",
    "\n",
    "        for key, value in self.data.items():\n",
    "            if key == 'num_nodes':\n",
    "                out.num_nodes = node_length\n",
    "            elif self.data.is_node_attr(key):\n",
    "                cat_dim = self.data.__cat_dim__(key, value)\n",
    "                out[key] = narrow(value, cat_dim, node_start, node_length)\n",
    "            elif self.data.is_edge_attr(key):\n",
    "                cat_dim = self.data.__cat_dim__(key, value)\n",
    "                out[key] = narrow(value, cat_dim, edge_start, edge_length)\n",
    "                if not self.keep_inter_cluster_edges:\n",
    "                    out[key] = out[key][edge_mask]\n",
    "\n",
    "        out.edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}({self.num_parts})'\n",
    "\n",
    "\n",
    "class ClusterLoader(torch.utils.data.DataLoader):\n",
    "    r\"\"\"The data loader scheme from the `\"Cluster-GCN: An Efficient Algorithm\n",
    "    for Training Deep and Large Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1905.07953>`_ paper which merges partioned subgraphs\n",
    "    and their between-cluster links from a large-scale graph data object to\n",
    "    form a mini-batch.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        Use :class:`~torch_geometric.loader.ClusterData` and\n",
    "        :class:`~torch_geometric.loader.ClusterLoader` in conjunction to\n",
    "        form mini-batches of clusters.\n",
    "        For an example of using Cluster-GCN, see\n",
    "        `examples/cluster_gcn_reddit.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/cluster_gcn_reddit.py>`_ or\n",
    "        `examples/cluster_gcn_ppi.py <https://github.com/pyg-team/\n",
    "        pytorch_geometric/blob/master/examples/cluster_gcn_ppi.py>`_.\n",
    "\n",
    "    Args:\n",
    "        cluster_data (torch_geometric.loader.ClusterData): The already\n",
    "            partioned data object.\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch.utils.data.DataLoader`, such as :obj:`batch_size`,\n",
    "            :obj:`shuffle`, :obj:`drop_last` or :obj:`num_workers`.\n",
    "    \"\"\"\n",
    "    def __init__(self, cluster_data, **kwargs):\n",
    "        self.cluster_data = cluster_data\n",
    "        iterator = range(len(cluster_data))\n",
    "        super().__init__(iterator, collate_fn=self._collate, **kwargs)\n",
    "\n",
    "    def _collate(self, batch: List[int]) -> Data:\n",
    "        if not isinstance(batch, torch.Tensor):\n",
    "            batch = torch.tensor(batch)\n",
    "\n",
    "        global_indptr = self.cluster_data.partition.indptr\n",
    "        global_index = self.cluster_data.partition.index\n",
    "\n",
    "        # Get all node-level and edge-level start and end indices for the\n",
    "        # current mini-batch:\n",
    "        node_start = self.cluster_data.partition.partptr[batch]\n",
    "        node_end = self.cluster_data.partition.partptr[batch + 1]\n",
    "        edge_start = global_indptr[node_start]\n",
    "        edge_end = global_indptr[node_end]\n",
    "\n",
    "        # Iterate over each partition in the batch and calculate new edge\n",
    "        # connectivity. This is done by slicing the corresponding source and\n",
    "        # destination indices for each partition and adjusting their indices to\n",
    "        # start from zero:\n",
    "        rows, cols, nodes, cumsum = [], [], [], 0\n",
    "        for i in range(batch.numel()):\n",
    "            nodes.append(torch.arange(node_start[i], node_end[i]))\n",
    "            indptr = global_indptr[node_start[i]:node_end[i] + 1]\n",
    "            indptr = indptr - edge_start[i]\n",
    "            if self.cluster_data.partition.sparse_format == 'csr':\n",
    "                row = ptr2index(indptr) + cumsum\n",
    "                col = global_index[edge_start[i]:edge_end[i]]\n",
    "\n",
    "            else:\n",
    "                col = ptr2index(indptr) + cumsum\n",
    "                row = global_index[edge_start[i]:edge_end[i]]\n",
    "\n",
    "            rows.append(row)\n",
    "            cols.append(col)\n",
    "            cumsum += indptr.numel() - 1\n",
    "\n",
    "        node = torch.cat(nodes, dim=0)\n",
    "        row = torch.cat(rows, dim=0)\n",
    "        col = torch.cat(cols, dim=0)\n",
    "\n",
    "        # Map `col` vector to valid entries and remove any entries that do not\n",
    "        # connect two nodes within the same mini-batch:\n",
    "        if self.cluster_data.partition.sparse_format == 'csr':\n",
    "            col, edge_mask = map_index(col, node)\n",
    "            row = row[edge_mask]\n",
    "        else:\n",
    "            row, edge_mask = map_index(row, node)\n",
    "            col = col[edge_mask]\n",
    "        out = copy.copy(self.cluster_data.data)\n",
    "\n",
    "        # Slice node-level and edge-level attributes according to its offsets:\n",
    "        for key, value in self.cluster_data.data.items():\n",
    "            if key == 'num_nodes':\n",
    "                out.num_nodes = cumsum\n",
    "            elif self.cluster_data.data.is_node_attr(key):\n",
    "                cat_dim = self.cluster_data.data.__cat_dim__(key, value)\n",
    "                out[key] = torch.cat([\n",
    "                    narrow(out[key], cat_dim, s, e - s)\n",
    "                    for s, e in zip(node_start, node_end)\n",
    "                ], dim=cat_dim)\n",
    "            elif self.cluster_data.data.is_edge_attr(key):\n",
    "                cat_dim = self.cluster_data.data.__cat_dim__(key, value)\n",
    "                value = torch.cat([\n",
    "                    narrow(out[key], cat_dim, s, e - s)\n",
    "                    for s, e in zip(edge_start, edge_end)\n",
    "                ], dim=cat_dim)\n",
    "                out[key] = select(value, edge_mask, dim=cat_dim)\n",
    "\n",
    "        out.edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBN2pGDueDpZ",
    "outputId": "7139ed04-cf1b-4406-80e3-f6b3ac10f05c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PubMed():\n",
      "==================\n",
      "Number of graphs: 1\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "\n",
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='data/Planetoid', name='PubMed',\n",
    "                    transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('==================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "\n",
    "# from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=32,\n",
    "                             shuffle=True)  # 2. Stochastic partioning scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbHL1x7fzjeR"
   },
   "source": [
    "## Define train, test, and run functions\n",
    "Here we define a simple `run` function for training the GNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "5pJQ7brC7VzC"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "data = data.to(device)\n",
    "\n",
    "def train(model, optimizer):\n",
    "    model.train()\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.01,\n",
    "    #                              weight_decay=5e-4)\n",
    "    for sub_data in train_loader:  # Iterate over each mini-batch.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        sub_data = sub_data.to(device)\n",
    "        out = model(sub_data.x,\n",
    "                    sub_data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(\n",
    "            out[sub_data.train_mask], sub_data.y[sub_data.train_mask]\n",
    "        )  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[\n",
    "            mask]  # Check against ground-truth labels.\n",
    "        accs.append(int(correct.sum()) /\n",
    "                    int(mask.sum()))  # Derive ratio of correct predictions.\n",
    "    return accs\n",
    "\n",
    "\n",
    "def run(model, optimizer, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model, optimizer)\n",
    "        train_acc, val_acc, test_acc = test(model)\n",
    "        print(\n",
    "            f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALbxt90iftTm"
   },
   "source": [
    "## Define a GNN class and Import Aggregations\n",
    "Now, let's define a GNN helper class and import all those new aggregation operators!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dhGCq1KbAxAX"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    Aggregation,\n",
    "    MaxAggregation,\n",
    "    MeanAggregation,\n",
    "    MultiAggregation,\n",
    "    SAGEConv,\n",
    "    SoftmaxAggregation,\n",
    "    StdAggregation,\n",
    "    SumAggregation,\n",
    "    VarAggregation,\n",
    ")\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, aggr='mean', aggr_kwargs=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(\n",
    "            dataset.num_node_features,\n",
    "            hidden_channels,\n",
    "            aggr=aggr,\n",
    "            aggr_kwargs=aggr_kwargs,\n",
    "        )\n",
    "        self.conv2 = SAGEConv(\n",
    "            hidden_channels,\n",
    "            dataset.num_classes,\n",
    "            aggr=copy.deepcopy(aggr),\n",
    "            aggr_kwargs=aggr_kwargs,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIg70vDmLO5V"
   },
   "source": [
    "### Original interface with string type as the aggregation argument\n",
    "Previously, PyG only supports customizing [MessagePassing](https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py) with simple aggregations (e.g., `'mean'`, `'max'`, `'sum'`). Let's define a GNN with `mean` aggregation and run it for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='hpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuRdM5VjDIAv",
    "outputId": "c9ca7892-8f37-425f-e884-791a86916fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=mean)\n",
      "  (conv2): SAGEConv(16, 3, aggr=mean)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train: 0.4167, Val Acc: 0.2220, Test Acc: 0.2140\n",
      "Epoch: 001, Train: 0.6167, Val Acc: 0.4040, Test Acc: 0.3920\n",
      "Epoch: 002, Train: 0.7833, Val Acc: 0.5220, Test Acc: 0.5080\n",
      "Epoch: 003, Train: 0.8833, Val Acc: 0.6360, Test Acc: 0.6340\n",
      "Epoch: 004, Train: 0.9000, Val Acc: 0.6820, Test Acc: 0.6730\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr='mean').to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "model = torch.compile(model,backend=\"hpu_backend\")\n",
    "\n",
    "run(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-e0uXR9Ldcf"
   },
   "source": [
    "### Use a single aggregation module as the aggregation argument\n",
    "In the new interface, the [MessagePassing](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.MessagePassing) class can take an [Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.Aggregation) module as an argument. Here we can define the mean aggregation by `MeanAggregation`. We can see the model achieves the same performance as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C66kK8wfCSnF",
    "outputId": "b59369ad-c612-4ec3-c199-732339a656e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=MeanAggregation())\n",
      "  (conv2): SAGEConv(16, 3, aggr=MeanAggregation())\n",
      ")\n",
      "Epoch: 000, Train: 0.4667, Val Acc: 0.2240, Test Acc: 0.2130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: aten::dropout: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /npu-stack/pytorch-fork/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.5167, Val Acc: 0.2820, Test Acc: 0.2920\n",
      "Epoch: 002, Train: 0.8333, Val Acc: 0.5420, Test Acc: 0.5390\n",
      "Epoch: 003, Train: 0.9000, Val Acc: 0.6360, Test Acc: 0.6150\n",
      "Epoch: 004, Train: 0.9000, Val Acc: 0.6720, Test Acc: 0.6470\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=MeanAggregation()).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "torch.compile(model,backend=\"hpu_backend\")\n",
    "run(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yslL-7_vLmaD"
   },
   "source": [
    "### Use a list of aggregation strings as the aggregation argument\n",
    "\n",
    "For defining multiple aggregations, we can use a list of strings as the input argument. The aggregations will be **resolved from pure strings** via a lookup table, following the design principles of the [class-resolver](https://github.com/cthoyt/class-resolver) library, e.g., by simply passing in `\"mean\"` to the [**MessagePassing**](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.MessagePassing) module. This will automatically resolve it to the MeanAggregation class. Let's see how a PNA-like GNN ([Corso et al. (2020)](https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf)) works. It converges much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ANwoZ7Z-aau",
    "outputId": "4f2abae7-a71c-413d-9d7f-75caaa58c4d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['mean', 'max', 'sum', 'std', 'var'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['mean', 'max', 'sum', 'std', 'var'])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train: 0.8167, Val Acc: 0.7040, Test Acc: 0.6900\n",
      "Epoch: 001, Train: 0.8667, Val Acc: 0.6860, Test Acc: 0.6850\n",
      "Epoch: 002, Train: 0.9333, Val Acc: 0.7240, Test Acc: 0.7170\n",
      "Epoch: 003, Train: 0.9333, Val Acc: 0.7340, Test Acc: 0.7290\n",
      "Epoch: 004, Train: 0.9667, Val Acc: 0.7560, Test Acc: 0.7360\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=['mean', 'max', 'sum', 'std', 'var']).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "model = torch.compile(model,backend=\"hpu_backend\")\n",
    "run(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cykfCUUhLojw"
   },
   "source": [
    "### Use a list of aggregation modules as the aggregation argument\n",
    "You can also use a list of [Aggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.Aggregation) modules to specify your convolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0M1UdXiOGpuC",
    "outputId": "ad4aad9b-06e8-40fe-ebe7-6578655df300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['MeanAggregation()', 'MaxAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['MeanAggregation()', 'MaxAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train: 0.8167, Val Acc: 0.7040, Test Acc: 0.6900\n",
      "Epoch: 001, Train: 0.8667, Val Acc: 0.6860, Test Acc: 0.6840\n",
      "Epoch: 002, Train: 0.9333, Val Acc: 0.7260, Test Acc: 0.7180\n",
      "Epoch: 003, Train: 0.9167, Val Acc: 0.7340, Test Acc: 0.7300\n",
      "Epoch: 004, Train: 0.9667, Val Acc: 0.7560, Test Acc: 0.7380\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(\n",
    "    16, aggr=[\n",
    "        MeanAggregation(),\n",
    "        MaxAggregation(),\n",
    "        SumAggregation(),\n",
    "        StdAggregation(),\n",
    "        VarAggregation(),\n",
    "    ]).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "model = torch.compile(model,backend=\"hpu_backend\")\n",
    "run(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xf2Qb_ydL2w2"
   },
   "source": [
    "### Use a list of mixed modules and strings as the aggregation argument\n",
    "And the mix of them is supported as well for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uLPbAHIHvYr",
    "outputId": "cf8adf13-6fca-46e7-8bd5-ce51a16e4abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['mean', 'MaxAggregation()', 'sum', 'StdAggregation()', 'var'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['mean', 'MaxAggregation()', 'sum', 'StdAggregation()', 'var'])\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train: 0.8167, Val Acc: 0.7040, Test Acc: 0.6900\n",
      "Epoch: 001, Train: 0.8667, Val Acc: 0.6860, Test Acc: 0.6840\n",
      "Epoch: 002, Train: 0.9333, Val Acc: 0.7240, Test Acc: 0.7170\n",
      "Epoch: 003, Train: 0.9333, Val Acc: 0.7340, Test Acc: 0.7280\n",
      "Epoch: 004, Train: 0.9667, Val Acc: 0.7560, Test Acc: 0.7370\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=[\n",
    "    'mean',\n",
    "    MaxAggregation(),\n",
    "    'sum',\n",
    "    StdAggregation(),\n",
    "    'var',\n",
    "]).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "model = torch.compile(model,backend=\"hpu_backend\")\n",
    "run(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFw0MMx3L-rd"
   },
   "source": [
    "### Define multiple aggregations with `MultiAggregation` module\n",
    "\n",
    "When a list is taken, [MessagePassing](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.MessagePassing) would stack these aggregators in via the [MultiAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.MultiAggregation) module automatically. But you can also directly pass a [MultiAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.MultiAggregation) instead of a list. Now let's see how can we define multiple aggregations with [MultiAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.MultiAggregation). Here we use different initial temperatures for [SoftmaxAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.SoftmaxAggregation) ([Li et al. (2020)](https://arxiv.org/abs/2006.07739)). Every different temperature will result in aggregation with different softness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6iUX1IvICOW",
    "outputId": "1f4da92e-9b35-49fe-f020-e42dbb3757a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=MultiAggregation([\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "  ], mode=cat))\n",
      "  (conv2): SAGEConv(16, 3, aggr=MultiAggregation([\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "  ], mode=cat))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train: 0.7000, Val Acc: 0.6300, Test Acc: 0.6330\n",
      "Epoch: 001, Train: 0.9333, Val Acc: 0.7740, Test Acc: 0.7090\n",
      "Epoch: 002, Train: 0.9500, Val Acc: 0.7720, Test Acc: 0.7210\n",
      "Epoch: 003, Train: 0.9500, Val Acc: 0.7660, Test Acc: 0.7190\n",
      "Epoch: 004, Train: 0.9667, Val Acc: 0.7540, Test Acc: 0.7210\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "aggr = MultiAggregation([\n",
    "    SoftmaxAggregation(t=0.01, learn=True),\n",
    "    SoftmaxAggregation(t=1, learn=True),\n",
    "    SoftmaxAggregation(t=100, learn=True),\n",
    "])\n",
    "model = GNN(16, aggr=aggr).to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "model = torch.compile(model,backend=\"hpu_backend\")\n",
    "run(model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHlCxX2LbLe2"
   },
   "source": [
    "What is more?\n",
    "There are many other aggregation operators supported for you to \"lego\" your GNNs. [PowerMeanAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.PowerMeanAggregation) allows you to define and potentially learn generalized means beyond simple  arithmetic mean such as harmonic mean and geometric mean. [LSTMAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.LSTMAggregation) can perform permutation-variant aggregation. More other interesting aggregation operators such as [Set2Set](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.Set2Set), [DegreeScalerAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.DegreeScalerAggregation), [SortAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.SortAggregation), [GraphMultisetTransformer](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.GraphMultisetTransformer), [AttentionalAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.AttentionalAggregation) and [EquilibriumAggregation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.aggr.EquilibriumAggregation) are ready for you to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDOmdUe0C3U1"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have been presented with the `torch_geometric.nn.aggr` package which provides a flexible interface to experiment with different aggregation functions with your message passing convolutions and unifies aggregation within GNNs across [`MessagePassing`](https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/nn/conv/message_passing.py) and [global readouts](https://github.com/pyg-team/pytorch_geometric/tree/master/torch_geometric/nn/glob). This new abstraction also makes designing new types of aggregation functions easier. Now, you can create your own aggregation function with the base `Aggregation` class. Please refer to the [docs](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/aggr/base.html#Aggregation) for more details.\n",
    "\n",
    "```python\n",
    "class MyAggregation(Aggregation):\n",
    "    def __init__(self, ...):\n",
    "      ...\n",
    "\n",
    "    def forward(self, x: Tensor, index: Optional[Tensor] = None,\n",
    "                ptr: Optional[Tensor] = None, dim_size: Optional[int] = None,\n",
    "                dim: int = -2) -> Tensor:\n",
    "      ...\n",
    "```\n",
    "\n",
    "*Have fun!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aYk1EAZweU2y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
