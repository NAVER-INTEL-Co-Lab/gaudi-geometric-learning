{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial5: Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will override the aggregation method of the GIN convolution module of Pytorch Geometric implementing the following methods:\n",
    "\n",
    "- Principal Neighborhood Aggregation (PNA)\n",
    "- Learning Aggregation Functions (LAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_HPU_LAZY_MODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0a0+git74cd574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the eager mode\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '0'\n",
    "\n",
    "# Verify the environment variable is set\n",
    "print(f\"PT_HPU_LAZY_MODE: {os.environ['PT_HPU_LAZY_MODE']}\")\n",
    "\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fea1def09f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Passing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUPPORTS_FUSED_EDGE_INDEX',\n",
       " 'T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_check_input',\n",
       " '_collect',\n",
       " '_compiled_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_edge_updater_signature',\n",
       " '_get_name',\n",
       " '_get_propagate_signature',\n",
       " '_index_select',\n",
       " '_index_select_safe',\n",
       " '_lift',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_jittable_templates',\n",
       " '_set_size',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'aggregate',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decomposed_layers',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'edge_update',\n",
       " 'edge_updater',\n",
       " 'eval',\n",
       " 'explain',\n",
       " 'explain_message',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'jittable',\n",
       " 'load_state_dict',\n",
       " 'message',\n",
       " 'message_and_aggregate',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'propagate',\n",
       " 'register_aggregate_forward_hook',\n",
       " 'register_aggregate_forward_pre_hook',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_edge_update_forward_hook',\n",
       " 'register_edge_update_forward_pre_hook',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_message_and_aggregate_forward_hook',\n",
       " 'register_message_and_aggregate_forward_pre_hook',\n",
       " 'register_message_forward_hook',\n",
       " 'register_message_forward_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_propagate_forward_hook',\n",
       " 'register_propagate_forward_pre_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'special_args',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'update',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(MessagePassing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the <span style='color:Blue'>aggregate</span> method, or, if you are using a sparse adjacency matrix, in the <span style='color:Blue'>message_and_aggregate</span> method. Convolutional classes in PyG extend MessagePassing, we construct our custom convoutional class extending GINConv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter operation in <span style='color:Blue'>aggregate</span>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rusty1s/pytorch_scatter/master/docs/source/_figures/add.svg?sanitize=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to change torch_scatter to the raw-pytorch implementation we implemented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('hpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter, Module, Sigmoid\n",
    "import torch\n",
    "# import torch_scatter\n",
    "from scatter_raw import scatter_add_raw as scatter_add\n",
    "from scatter_raw import broadcast\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "class AbstractLAFLayer(Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AbstractLAFLayer, self).__init__()\n",
    "        assert 'units' in kwargs or 'weights' in kwargs        \n",
    "        self.device = device\n",
    "        # self.ngpus = torch.cuda.device_count()\n",
    "        self.ngpus = 1\n",
    "        \n",
    "        if 'kernel_initializer' in kwargs.keys():\n",
    "            assert kwargs['kernel_initializer'] in [\n",
    "                'random_normal',\n",
    "                'glorot_normal',\n",
    "                'he_normal',\n",
    "                'random_uniform',\n",
    "                'glorot_uniform',\n",
    "                'he_uniform']\n",
    "            self.kernel_initializer = kwargs['kernel_initializer']\n",
    "        else:\n",
    "            self.kernel_initializer = 'random_normal'\n",
    "\n",
    "        if 'weights' in kwargs.keys():\n",
    "            self.weights = Parameter(kwargs['weights'].to(self.device), \\\n",
    "                                     requires_grad=True)\n",
    "            self.units = self.weights.shape[1]\n",
    "        else:\n",
    "            self.units = kwargs['units']\n",
    "            params = torch.empty(12, self.units, device=self.device)\n",
    "            if self.kernel_initializer == 'random_normal':\n",
    "                torch.nn.init.normal_(params)\n",
    "            elif self.kernel_initializer == 'glorot_normal':\n",
    "                torch.nn.init.xavier_normal_(params)\n",
    "            elif self.kernel_initializer == 'he_normal':\n",
    "                torch.nn.init.kaiming_normal_(params)\n",
    "            elif self.kernel_initializer == 'random_uniform':\n",
    "                torch.nn.init.uniform_(params)\n",
    "            elif self.kernel_initializer == 'glorot_uniform':\n",
    "                torch.nn.init.xavier_uniform_(params)\n",
    "            elif self.kernel_initializer == 'he_uniform':\n",
    "                torch.nn.init.kaiming_uniform_(params)\n",
    "            self.weights = Parameter(params, \\\n",
    "                                     requires_grad=True)\n",
    "        e = torch.tensor([1,-1,1,-1], dtype=torch.float32, device=self.device)\n",
    "        self.e = Parameter(e, requires_grad=False)\n",
    "        num_idx = torch.tensor([1,1,0,0], dtype=torch.float32, device=self.device).\\\n",
    "                                view(1,1,-1,1)\n",
    "        self.num_idx = Parameter(num_idx, requires_grad=False)\n",
    "        den_idx = torch.tensor([0,0,1,1], dtype=torch.float32, device=self.device).\\\n",
    "                                view(1,1,-1,1)\n",
    "        self.den_idx = Parameter(den_idx, requires_grad=False)\n",
    "\n",
    "\n",
    "class LAFLayer(AbstractLAFLayer):\n",
    "    def __init__(self, eps=1e-7, **kwargs):\n",
    "        super(LAFLayer, self).__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, data, index, dim=0, **kwargs):\n",
    "        eps = self.eps\n",
    "        sup = 1.0 - eps \n",
    "        e = self.e\n",
    "\n",
    "        x = torch.clamp(data, eps, sup)\n",
    "        x = torch.unsqueeze(x, -1)\n",
    "        e = e.view(1,1,-1)        \n",
    "\n",
    "        exps = (1. - e)/2. + x*e \n",
    "        exps = torch.unsqueeze(exps, -1)\n",
    "        exps = torch.pow(exps, torch.relu(self.weights[0:4]))\n",
    "\n",
    "        # scatter = torch_scatter.scatter_add(exps, index.view(-1), dim=dim)\n",
    "        \n",
    "        # scatter = scatter_add(exps, index.view(-1), dim=dim)\n",
    "        \n",
    "        scatter_res = scatter(exps, index, dim=dim, reduce='sum')\n",
    "        \n",
    "        # size = torch.tensor(exps.size())\n",
    "        # size[dim] = index.max() + 1\n",
    "        # scatter = torch.zeros(*size, dtype=exps.dtype, device=exps.device)\n",
    "        # index_expand = broadcast(index, exps, dim)\n",
    "        # scatter.scatter_add_(dim, index_expand, exps)\n",
    "                \n",
    "        scatter_res = torch.clamp(scatter_res, eps)\n",
    "\n",
    "        sqrt = torch.pow(scatter_res, torch.relu(self.weights[4:8]))\n",
    "        alpha_beta = self.weights[8:12].view(1,1,4,-1)\n",
    "        terms = sqrt * alpha_beta\n",
    "\n",
    "        num = torch.sum(terms * self.num_idx, dim=2)\n",
    "        den = torch.sum(terms * self.den_idx, dim=2)\n",
    "        \n",
    "        multiplier = 2.0*torch.clamp(torch.sign(den), min=0.0) - 1.0\n",
    "\n",
    "        den = torch.where((den < eps) & (den > -eps), multiplier*eps, den)\n",
    "\n",
    "        res = num / den\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAF Aggregation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"laf.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLAFConv(GINConv):\n",
    "    def __init__(self, nn, units=1, node_dim=32, **kwargs):\n",
    "        super(GINLAFConv, self).__init__(nn, **kwargs)\n",
    "        self.laf = LAFLayer(units=units, kernel_initializer='random_uniform')\n",
    "        self.mlp = torch.nn.Linear(node_dim*units, node_dim)\n",
    "        self.dim = node_dim\n",
    "        self.units = units\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        x = torch.sigmoid(inputs)\n",
    "        x = self.laf(x, index)\n",
    "        x = x.view((-1, self.dim * self.units))\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNA Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pna.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scatter_raw import (\n",
    "    scatter_add_raw as scatter_add,\n",
    "    scatter_max_raw as scatter_max,\n",
    "    scatter_mean_raw as scatter_mean\n",
    ")\n",
    "\n",
    "class GINPNAConv(GINConv):\n",
    "    def __init__(self, nn, node_dim=32, **kwargs):\n",
    "        super(GINPNAConv, self).__init__(nn, **kwargs)\n",
    "        self.mlp = torch.nn.Linear(node_dim*12, node_dim)\n",
    "        self.delta = 2.5749\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        sums = scatter_add(inputs, index, dim=0)\n",
    "        maxs = scatter_max(inputs, index, dim=0)[0]\n",
    "        means = scatter_mean(inputs, index, dim=0)\n",
    "        var = torch.relu(scatter_mean(inputs ** 2, index, dim=0) - means ** 2)\n",
    "        \n",
    "        aggrs = [sums, maxs, means, var]\n",
    "        c_idx = index.bincount().float().view(-1, 1)\n",
    "        l_idx = torch.log(c_idx + 1.)\n",
    "        \n",
    "        amplification_scaler = [c_idx / self.delta * a for a in aggrs]\n",
    "        attenuation_scaler = [self.delta / c_idx * a for a in aggrs]\n",
    "        combinations = torch.cat(aggrs+ amplification_scaler+ attenuation_scaler, dim=1)\n",
    "        x = self.mlp(combinations)\n",
    "    \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing, SAGEConv, GINConv, global_add_pool\n",
    "# import torch_scatter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import os.path as osp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "path = osp.join('./', 'data', 'TU')\n",
    "dataset = TUDataset(path, name='MUTAG').shuffle()\n",
    "test_dataset = dataset[:len(dataset) // 10]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LAFNet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "        units = 3\n",
    "        \n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINLAFConv(nn1, units=units, node_dim=num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINLAFConv(nn2, units=units, node_dim=dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINLAFConv(nn3, units=units, node_dim=dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINLAFConv(nn4, units=units, node_dim=dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINLAFConv(nn5, units=units, node_dim=dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNANet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "\n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINPNAConv(nn1, node_dim=num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINPNAConv(nn2, node_dim=dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINPNAConv(nn3, node_dim=dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINPNAConv(nn4, node_dim=dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINPNAConv(nn5, node_dim=dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GINNet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "\n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 2113407800 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ac66a6fc0c48dbb106859204282e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1105 06:36:04.612000 140646669711360 torch/_dynamo/variables/tensor.py:754] [0/0] Graph break from `Tensor.item()`, consider setting:\n",
      "W1105 06:36:04.612000 140646669711360 torch/_dynamo/variables/tensor.py:754] [0/0]     torch._dynamo.config.capture_scalar_outputs = True\n",
      "W1105 06:36:04.612000 140646669711360 torch/_dynamo/variables/tensor.py:754] [0/0] or:\n",
      "W1105 06:36:04.612000 140646669711360 torch/_dynamo/variables/tensor.py:754] [0/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1\n",
      "W1105 06:36:04.612000 140646669711360 torch/_dynamo/variables/tensor.py:754] [0/0] to include these operations in the captured graph.\n",
      "W1105 06:36:04.612000 140646669711360 torch/_dynamo/variables/tensor.py:754] [0/0] \n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\"> </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">47</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44 </span>optimizer = torch.optim.Adam(model.parameters(), lr=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0.001</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> epoch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> trange(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">101</span>):                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>47 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>train_loss = train(epoch)                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">48 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># with torch.inference_mode():</span>                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#     model.eval()</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">50 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">#     train_acc = test(train_loader)</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">27</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">24 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">25 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>output = model(data.x, data.edge_index, data.batch)                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>loss = F.nll_loss(output, data.y)                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>27 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>loss_all += loss.item() * data.num_graphs                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">30 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss_all / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(train_loader.dataset)                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">531</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 528 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 529 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 530 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 531 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 532 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 533 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 534 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">289</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat the same comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>289 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>_engine_run_backward(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>tensors,                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>grad_tensors_,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>retain_graph,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">768</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_engine_run_backward</span>      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">765 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attach_logging_hooks:                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">766 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">767 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>768 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to </span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">769 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>t_outputs, *args, **kwargs                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the backward pass</span>                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">function.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">306</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply</span>                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">303 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #808000; text-decoration-color: #808000\">\"of them.\"</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">304 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">305 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>user_fn = vjp_fn <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> vjp_fn <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> Function.vjp <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> backward_fn                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>306 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> user_fn(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args)                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">307 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">308 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_jvp</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args):                                                            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">309 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">      </span><span style=\"color: #808000; text-decoration-color: #808000\">r\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">runtime_wrappers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1861</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1858 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Pass args even though they're unused, so that the graph is built</span>    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1859 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>out = CompiledFunctionBackward.apply(*all_args)                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1860 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1861 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>out = call_compiled_backward()                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1862 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1863 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># TODO: figure out how to refactor the backward properly so I can use ao</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1864 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> CompiledFunction.maybe_subclass_metadata <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">runtime_wrappers.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1809</span>  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">call_compiled_backward</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1806 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                        </span>bw_module, placeholder_list                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1807 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                     </span>)                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1808 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1809 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>out = call_func_at_runtime_with_args(                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1810 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>CompiledFunction.compiled_bw,                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1811 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>all_args,                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1812 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                  </span>steal_args=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">120</span> in           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">call_func_at_runtime_with_args</span>                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>context = torch._C._DisableAutocast <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> disable_amp <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> nullcontext                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> context():                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">119 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">hasattr</span>(f, <span style=\"color: #808000; text-decoration-color: #808000\">\"_boxed_call\"</span>):                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>120 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>out = normalize_as_list(f(args))                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># TODO: Please remove soon</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/_dynamo/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">eval_frame.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">600</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_fn</span>                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 597 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_fn</span>(*args, **kwargs):                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 598 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>prior = set_eval_frame(callback)                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 599 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 600 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> fn(*args, **kwargs)                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 601 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 602 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>set_eval_frame(prior)                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 603 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">94</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">g</span>          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 91 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 92 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">make_boxed_func</span>(f):                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 93 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">g</span>(args):                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span> 94 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> f(*args)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 96 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>g._boxed_call = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[attr-defined]</span>                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> g                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/fx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_lazy_graph_module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">124</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_lazy_forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">121 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># call `__call__` rather than 'forward' since recompilation may</span>                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">122 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># install a wrapper for `__call__` to provide a customized error</span>                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">123 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># message.</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>124 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(*args, **kwargs)                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">125 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">126 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>forward = _lazy_forward                                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">127 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/fx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph_module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">738</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">call_wrapped</span>             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">735 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>._wrapped_call = _WrappedCall(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>, cls_call)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[attr-defined</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">736 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">737 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">call_wrapped</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>738 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._wrapped_call(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs)                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">739 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">740 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span> = call_wrapped  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[method-assign]</span>                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">741 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/fx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph_module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">316</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">313 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>)                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">314 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e.with_traceback(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># noqa: B904</span>                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">315 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>316 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> e                                                                    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">317 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">318 </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@compatibility</span>(is_backward_compatible=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">319 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">class</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; text-decoration: underline\">GraphModule</span>(torch.nn.Module):                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/fx/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph_module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">303</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">300 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cls_call <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">301 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cls_call(obj, *args, **kwargs)                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">302 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>303 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">super</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cls, obj).<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[mi</span>   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">304 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">305 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> e.__traceback__                                                         <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">306 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>topmost_framesummary: traceback.FrameSummary = (                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1556</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_wrapped_call_impl</span>    <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1553 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1554 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._compiled_call_impl(*args, **kwargs)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># type: ignore[misc]</span>        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1555 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1556 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._call_impl(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1557 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1558 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, *args, **kwargs):                                                <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1559 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>forward_call = (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._slow_forward <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch._C._get_tracing_state() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fo  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1565</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1562 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1563 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1564 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>1565 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">      </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1568 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>result = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/dynamo/compile_backend/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">recipe_co</span> <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">mpiler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">242</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">__call__</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._range_list.insert(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, RangeInfo([<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], [<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], <span style=\"color: #808000; text-decoration-color: #808000\">\"1\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"1\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>))               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._range_list.insert(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, RangeInfo([<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], [<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], <span style=\"color: #808000; text-decoration-color: #808000\">\"1\"</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"1\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>))               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">241 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span> <span style=\"color: #800000; text-decoration-color: #800000\"> </span>242 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">         </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._recipe_id = graph_compile(                                               <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">243 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>graph=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._jit_ir.graph,                                                  <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">244 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>inputs=inputs,                                                             <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>dynamic=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._dynamic,                                                     <span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\"></span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>Input sizes must be equal\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m\u001b[0m\u001b[31m\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m\u001b[0m\u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m47\u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m44 \u001b[0moptimizer = torch.optim.Adam(model.parameters(), lr=\u001b[94m0.001\u001b[0m)                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m45 \u001b[0m                                                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m46 \u001b[0m\u001b[94mfor\u001b[0m epoch \u001b[95min\u001b[0m trange(\u001b[94m1\u001b[0m, \u001b[94m101\u001b[0m):                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m47 \u001b[2m   \u001b[0mtrain_loss = train(epoch)                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m48 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# with torch.inference_mode():\u001b[0m                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m49 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m#     model.eval()\u001b[0m                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m50 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m#     train_acc = test(train_loader)\u001b[0m                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mtrain\u001b[0m:\u001b[94m27\u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m24 \u001b[0m\u001b[2m      \u001b[0moptimizer.zero_grad()                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m25 \u001b[0m\u001b[2m      \u001b[0moutput = model(data.x, data.edge_index, data.batch)                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m26 \u001b[0m\u001b[2m      \u001b[0mloss = F.nll_loss(output, data.y)                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m27 \u001b[2m      \u001b[0mloss.backward()                                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m28 \u001b[0m\u001b[2m      \u001b[0mloss_all += loss.item() * data.num_graphs                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m29 \u001b[0m\u001b[2m      \u001b[0moptimizer.step()                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m30 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mreturn\u001b[0m loss_all / \u001b[96mlen\u001b[0m(train_loader.dataset)                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m531\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 528 \u001b[0m\u001b[2m            \u001b[0mcreate_graph=create_graph,                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 529 \u001b[0m\u001b[2m            \u001b[0minputs=inputs,                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 530 \u001b[0m\u001b[2m         \u001b[0m)                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 531 \u001b[2m      \u001b[0mtorch.autograd.backward(                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 532 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 533 \u001b[0m\u001b[2m      \u001b[0m)                                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 534 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m289\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# The reason we repeat the same comment below is that\u001b[0m                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m289 \u001b[2m   \u001b[0m_engine_run_backward(                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m      \u001b[0mtensors,                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m      \u001b[0mgrad_tensors_,                                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m      \u001b[0mretain_graph,                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33mgraph.py\u001b[0m:\u001b[94m768\u001b[0m in \u001b[92m_engine_run_backward\u001b[0m      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m765 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mif\u001b[0m attach_logging_hooks:                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m766 \u001b[0m\u001b[2m      \u001b[0munregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m767 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m768 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m Variable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to \u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m769 \u001b[0m\u001b[2m         \u001b[0mt_outputs, *args, **kwargs                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m770 \u001b[0m\u001b[2m      \u001b[0m)  \u001b[2m# Calls into the C++ engine to run the backward pass\u001b[0m                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m771 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33mfunction.py\u001b[0m:\u001b[94m306\u001b[0m in \u001b[92mapply\u001b[0m                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m303 \u001b[0m\u001b[2m            \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mof them.\u001b[0m\u001b[33m\"\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m304 \u001b[0m\u001b[2m         \u001b[0m)                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m305 \u001b[0m\u001b[2m      \u001b[0muser_fn = vjp_fn \u001b[94mif\u001b[0m vjp_fn \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m Function.vjp \u001b[94melse\u001b[0m backward_fn                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m306 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m user_fn(\u001b[96mself\u001b[0m, *args)                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m307 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m308 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mapply_jvp\u001b[0m(\u001b[96mself\u001b[0m, *args):                                                            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m309 \u001b[0m\u001b[2;90m      \u001b[0m\u001b[33mr\u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/\u001b[0m\u001b[1;33mruntime_wrappers.py\u001b[0m:\u001b[94m1861\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mbackward\u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1858 \u001b[0m\u001b[2m               \u001b[0m\u001b[2m# Pass args even though they're unused, so that the graph is built\u001b[0m    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1859 \u001b[0m\u001b[2m               \u001b[0mout = CompiledFunctionBackward.apply(*all_args)                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1860 \u001b[0m\u001b[2m            \u001b[0m\u001b[94melse\u001b[0m:                                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1861 \u001b[2m               \u001b[0mout = call_compiled_backward()                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1862 \u001b[0m\u001b[2m            \u001b[0m                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1863 \u001b[0m\u001b[2m            \u001b[0m\u001b[2m# TODO: figure out how to refactor the backward properly so I can use ao\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1864 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mif\u001b[0m CompiledFunction.maybe_subclass_metadata \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/\u001b[0m\u001b[1;33mruntime_wrappers.py\u001b[0m:\u001b[94m1809\u001b[0m  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mcall_compiled_backward\u001b[0m                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1806 \u001b[0m\u001b[2m                        \u001b[0mbw_module, placeholder_list                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1807 \u001b[0m\u001b[2m                     \u001b[0m)                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1808 \u001b[0m\u001b[2m               \u001b[0m                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1809 \u001b[2m               \u001b[0mout = call_func_at_runtime_with_args(                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1810 \u001b[0m\u001b[2m                  \u001b[0mCompiledFunction.compiled_bw,                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1811 \u001b[0m\u001b[2m                  \u001b[0mall_args,                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1812 \u001b[0m\u001b[2m                  \u001b[0msteal_args=\u001b[94mTrue\u001b[0m,                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m120\u001b[0m in           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[92mcall_func_at_runtime_with_args\u001b[0m                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m   \u001b[0mcontext = torch._C._DisableAutocast \u001b[94mif\u001b[0m disable_amp \u001b[94melse\u001b[0m nullcontext                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m118 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mwith\u001b[0m context():                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m119 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mhasattr\u001b[0m(f, \u001b[33m\"\u001b[0m\u001b[33m_boxed_call\u001b[0m\u001b[33m\"\u001b[0m):                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m120 \u001b[2m         \u001b[0mout = normalize_as_list(f(args))                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m         \u001b[0m\u001b[2m# TODO: Please remove soon\u001b[0m                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m         \u001b[0m\u001b[2m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/\u001b[0m\u001b[1;33meval_frame.py\u001b[0m:\u001b[94m600\u001b[0m in \u001b[92m_fn\u001b[0m                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 597 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_fn\u001b[0m(*args, **kwargs):                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 598 \u001b[0m\u001b[2m         \u001b[0mprior = set_eval_frame(callback)                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 599 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 600 \u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m fn(*args, **kwargs)                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 601 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mfinally\u001b[0m:                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 602 \u001b[0m\u001b[2m            \u001b[0mset_eval_frame(prior)                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 603 \u001b[0m                                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/_functorch/_aot_autograd/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m94\u001b[0m in \u001b[92mg\u001b[0m          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 91 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 92 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmake_boxed_func\u001b[0m(f):                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 93 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mg\u001b[0m(args):                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m 94 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m f(*args)                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 96 \u001b[0m\u001b[2m   \u001b[0mg._boxed_call = \u001b[94mTrue\u001b[0m  \u001b[2m# type: ignore[attr-defined]\u001b[0m                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mreturn\u001b[0m g                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/fx/\u001b[0m\u001b[1;33m_lazy_graph_module.py\u001b[0m:\u001b[94m124\u001b[0m in \u001b[92m_lazy_forward\u001b[0m      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m121 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# call `__call__` rather than 'forward' since recompilation may\u001b[0m                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m122 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# install a wrapper for `__call__` to provide a customized error\u001b[0m                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m123 \u001b[0m\u001b[2m      \u001b[0m\u001b[2m# message.\u001b[0m                                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m124 \u001b[2m      \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m(*args, **kwargs)                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m125 \u001b[0m\u001b[2m   \u001b[0m                                                                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m126 \u001b[0m\u001b[2m   \u001b[0mforward = _lazy_forward                                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m127 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/fx/\u001b[0m\u001b[1;33mgraph_module.py\u001b[0m:\u001b[94m738\u001b[0m in \u001b[92mcall_wrapped\u001b[0m             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m735 \u001b[0m\u001b[2m         \u001b[0m\u001b[96mcls\u001b[0m._wrapped_call = _WrappedCall(\u001b[96mcls\u001b[0m, cls_call)  \u001b[2m# type: ignore[attr-defined\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m736 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m737 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcall_wrapped\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m738 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._wrapped_call(\u001b[96mself\u001b[0m, *args, **kwargs)                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m739 \u001b[0m\u001b[2m      \u001b[0m                                                                                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m740 \u001b[0m\u001b[2m      \u001b[0m\u001b[96mcls\u001b[0m.\u001b[92m__call__\u001b[0m = call_wrapped  \u001b[2m# type: ignore[method-assign]\u001b[0m                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m741 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/fx/\u001b[0m\u001b[1;33mgraph_module.py\u001b[0m:\u001b[94m316\u001b[0m in \u001b[92m__call__\u001b[0m                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m313 \u001b[0m\u001b[2m            \u001b[0m)                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m314 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mraise\u001b[0m e.with_traceback(\u001b[94mNone\u001b[0m)  \u001b[2m# noqa: B904\u001b[0m                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m315 \u001b[0m\u001b[2m         \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m316 \u001b[2m            \u001b[0m\u001b[94mraise\u001b[0m e                                                                    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m317 \u001b[0m                                                                                           \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m318 \u001b[0m\u001b[1;95m@compatibility\u001b[0m(is_backward_compatible=\u001b[94mTrue\u001b[0m)                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m319 \u001b[0m\u001b[94mclass\u001b[0m \u001b[4;92mGraphModule\u001b[0m(torch.nn.Module):                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/fx/\u001b[0m\u001b[1;33mgraph_module.py\u001b[0m:\u001b[94m303\u001b[0m in \u001b[92m__call__\u001b[0m                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m300 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.cls_call \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m301 \u001b[0m\u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.cls_call(obj, *args, **kwargs)                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m302 \u001b[0m\u001b[2m         \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m303 \u001b[2m            \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m(\u001b[96mself\u001b[0m.cls, obj).\u001b[92m__call__\u001b[0m(*args, **kwargs)  \u001b[2m# type: ignore[mi\u001b[0m   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m304 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m305 \u001b[0m\u001b[2m         \u001b[0m\u001b[94massert\u001b[0m e.__traceback__                                                         \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m306 \u001b[0m\u001b[2m         \u001b[0mtopmost_framesummary: traceback.FrameSummary = (                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1556\u001b[0m in \u001b[92m_wrapped_call_impl\u001b[0m    \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1553 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1554 \u001b[0m\u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._compiled_call_impl(*args, **kwargs)  \u001b[2m# type: ignore[misc]\u001b[0m        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1555 \u001b[0m\u001b[2m      \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1556 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._call_impl(*args, **kwargs)                                       \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1557 \u001b[0m\u001b[2m   \u001b[0m                                                                                      \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1558 \u001b[0m\u001b[2m   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_call_impl\u001b[0m(\u001b[96mself\u001b[0m, *args, **kwargs):                                                \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1559 \u001b[0m\u001b[2m      \u001b[0mforward_call = (\u001b[96mself\u001b[0m._slow_forward \u001b[94mif\u001b[0m torch._C._get_tracing_state() \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.fo  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1565\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1562 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1563 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1564 \u001b[0m\u001b[2m            \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m1565 \u001b[2m         \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1566 \u001b[0m\u001b[2m      \u001b[0m                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1567 \u001b[0m\u001b[2m      \u001b[0m\u001b[94mtry\u001b[0m:                                                                              \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m1568 \u001b[0m\u001b[2m         \u001b[0mresult = \u001b[94mNone\u001b[0m                                                                 \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m5\u001b[0m                                                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/habana_frameworks/torch/dynamo/compile_backend/\u001b[0m\u001b[1;33mrecipe_co\u001b[0m \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[1;33mmpiler.py\u001b[0m:\u001b[94m242\u001b[0m in \u001b[92m__call__\u001b[0m                                                                        \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m                                                                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m239 \u001b[0m\u001b[2m            \u001b[0m\u001b[96mself\u001b[0m._range_list.insert(\u001b[94m0\u001b[0m, RangeInfo([\u001b[94m1\u001b[0m], [\u001b[94m1\u001b[0m], \u001b[33m\"\u001b[0m\u001b[33m1\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33m1\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94m0\u001b[0m))               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m240 \u001b[0m\u001b[2m            \u001b[0m\u001b[96mself\u001b[0m._range_list.insert(\u001b[94m1\u001b[0m, RangeInfo([\u001b[94m1\u001b[0m], [\u001b[94m1\u001b[0m], \u001b[33m\"\u001b[0m\u001b[33m1\u001b[0m\u001b[33m\"\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33m1\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94m1\u001b[0m))               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m241 \u001b[0m\u001b[2m         \u001b[0m                                                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m \u001b[31m \u001b[0m242 \u001b[2m         \u001b[0m\u001b[96mself\u001b[0m._recipe_id = graph_compile(                                               \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m243 \u001b[0m\u001b[2m            \u001b[0mgraph=\u001b[96mself\u001b[0m._jit_ir.graph,                                                  \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m244 \u001b[0m\u001b[2m            \u001b[0minputs=inputs,                                                             \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m   \u001b[2m245 \u001b[0m\u001b[2m            \u001b[0mdynamic=\u001b[96mself\u001b[0m._dynamic,                                                     \u001b[31m\u001b[0m\n",
       "\u001b[31m\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mInput sizes must be equal\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import traceback\n",
    "traceback.install()\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# device = torch.device(\"hpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "net = \"LAF\"\n",
    "if net == \"LAF\":\n",
    "    model = LAFNet().to(device)\n",
    "elif net == \"PNA\":\n",
    "    model = PNANet().to(device)\n",
    "elif net == \"GIN\":\n",
    "    model = GINNet().to(device)\n",
    "\n",
    "def train(epoch):\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y)        \n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "model.train()\n",
    "model = torch.compile(model, backend=\"hpu_backend\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in trange(1, 101):    \n",
    "    train_loss = train(epoch)    \n",
    "    # with torch.inference_mode():\n",
    "    #     model.eval()\n",
    "    #     train_acc = test(train_loader)\n",
    "    #     test_acc = test(test_loader)\n",
    "    #     print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
    "    #         'Train Acc: {:.7f}, Test Acc: {:.7f}'.format(epoch, train_loss,\n",
    "    #                                                     train_acc, test_acc))\n",
    "    # model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
