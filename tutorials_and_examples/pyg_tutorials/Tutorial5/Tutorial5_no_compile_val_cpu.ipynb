{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial5: Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will override the aggregation method of the GIN convolution module of Pytorch Geometric implementing the following methods:\n",
    "\n",
    "- Principal Neighborhood Aggregation (PNA)\n",
    "- Learning Aggregation Functions (LAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_HPU_LAZY_MODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0a0+git74cd574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the eager mode\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '0'\n",
    "\n",
    "# Verify the environment variable is set\n",
    "print(f\"PT_HPU_LAZY_MODE: {os.environ['PT_HPU_LAZY_MODE']}\")\n",
    "\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3b7ef649f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Passing Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUPPORTS_FUSED_EDGE_INDEX',\n",
       " 'T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_call_impl',\n",
       " '_check_input',\n",
       " '_collect',\n",
       " '_compiled_call_impl',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_edge_updater_signature',\n",
       " '_get_name',\n",
       " '_get_propagate_signature',\n",
       " '_index_select',\n",
       " '_index_select_safe',\n",
       " '_lift',\n",
       " '_load_from_state_dict',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_named_members',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_set_jittable_templates',\n",
       " '_set_size',\n",
       " '_slow_forward',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'add_module',\n",
       " 'aggregate',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'decomposed_layers',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'edge_update',\n",
       " 'edge_updater',\n",
       " 'eval',\n",
       " 'explain',\n",
       " 'explain_message',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'get_buffer',\n",
       " 'get_extra_state',\n",
       " 'get_parameter',\n",
       " 'get_submodule',\n",
       " 'half',\n",
       " 'ipu',\n",
       " 'jittable',\n",
       " 'load_state_dict',\n",
       " 'message',\n",
       " 'message_and_aggregate',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'propagate',\n",
       " 'register_aggregate_forward_hook',\n",
       " 'register_aggregate_forward_pre_hook',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_edge_update_forward_hook',\n",
       " 'register_edge_update_forward_pre_hook',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_message_and_aggregate_forward_hook',\n",
       " 'register_message_and_aggregate_forward_pre_hook',\n",
       " 'register_message_forward_hook',\n",
       " 'register_message_forward_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_propagate_forward_hook',\n",
       " 'register_propagate_forward_pre_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_parameters',\n",
       " 'set_extra_state',\n",
       " 'share_memory',\n",
       " 'special_args',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'type',\n",
       " 'update',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(MessagePassing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the <span style='color:Blue'>aggregate</span> method, or, if you are using a sparse adjacency matrix, in the <span style='color:Blue'>message_and_aggregate</span> method. Convolutional classes in PyG extend MessagePassing, we construct our custom convoutional class extending GINConv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scatter operation in <span style='color:Blue'>aggregate</span>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/rusty1s/pytorch_scatter/master/docs/source/_figures/add.svg?sanitize=true\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to change torch_scatter to the raw-pytorch implementation we implemented "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('hpu')\n",
    "device_cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Parameter, Module, Sigmoid\n",
    "import torch\n",
    "# import torch_scatter\n",
    "from scatter_raw import scatter_add_raw as scatter_add\n",
    "from scatter_raw import broadcast\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "class AbstractLAFLayer(Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AbstractLAFLayer, self).__init__()\n",
    "        assert 'units' in kwargs or 'weights' in kwargs        \n",
    "        self.device = device\n",
    "        # self.ngpus = torch.cuda.device_count()\n",
    "        self.ngpus = 1\n",
    "        \n",
    "        if 'kernel_initializer' in kwargs.keys():\n",
    "            assert kwargs['kernel_initializer'] in [\n",
    "                'random_normal',\n",
    "                'glorot_normal',\n",
    "                'he_normal',\n",
    "                'random_uniform',\n",
    "                'glorot_uniform',\n",
    "                'he_uniform']\n",
    "            self.kernel_initializer = kwargs['kernel_initializer']\n",
    "        else:\n",
    "            self.kernel_initializer = 'random_normal'\n",
    "\n",
    "        if 'weights' in kwargs.keys():\n",
    "            self.weights = Parameter(kwargs['weights'].to(self.device), \\\n",
    "                                     requires_grad=True)\n",
    "            self.units = self.weights.shape[1]\n",
    "        else:\n",
    "            self.units = kwargs['units']\n",
    "            params = torch.empty(12, self.units, device=self.device)\n",
    "            if self.kernel_initializer == 'random_normal':\n",
    "                torch.nn.init.normal_(params)\n",
    "            elif self.kernel_initializer == 'glorot_normal':\n",
    "                torch.nn.init.xavier_normal_(params)\n",
    "            elif self.kernel_initializer == 'he_normal':\n",
    "                torch.nn.init.kaiming_normal_(params)\n",
    "            elif self.kernel_initializer == 'random_uniform':\n",
    "                torch.nn.init.uniform_(params)\n",
    "            elif self.kernel_initializer == 'glorot_uniform':\n",
    "                torch.nn.init.xavier_uniform_(params)\n",
    "            elif self.kernel_initializer == 'he_uniform':\n",
    "                torch.nn.init.kaiming_uniform_(params)\n",
    "            self.weights = Parameter(params, \\\n",
    "                                     requires_grad=True)\n",
    "        e = torch.tensor([1,-1,1,-1], dtype=torch.float32, device=self.device)\n",
    "        self.e = Parameter(e, requires_grad=False)\n",
    "        num_idx = torch.tensor([1,1,0,0], dtype=torch.float32, device=self.device).\\\n",
    "                                view(1,1,-1,1)\n",
    "        self.num_idx = Parameter(num_idx, requires_grad=False)\n",
    "        den_idx = torch.tensor([0,0,1,1], dtype=torch.float32, device=self.device).\\\n",
    "                                view(1,1,-1,1)\n",
    "        self.den_idx = Parameter(den_idx, requires_grad=False)\n",
    "\n",
    "\n",
    "class LAFLayer(AbstractLAFLayer):\n",
    "    def __init__(self, eps=1e-7, **kwargs):\n",
    "        super(LAFLayer, self).__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, data, index, dim=0, **kwargs):\n",
    "        eps = self.eps\n",
    "        sup = 1.0 - eps \n",
    "        e = self.e\n",
    "\n",
    "        x = torch.clamp(data, eps, sup)\n",
    "        x = torch.unsqueeze(x, -1)\n",
    "        e = e.view(1,1,-1)        \n",
    "\n",
    "        exps = (1. - e)/2. + x*e \n",
    "        exps = torch.unsqueeze(exps, -1)\n",
    "        exps = torch.pow(exps, torch.relu(self.weights[0:4]))\n",
    "\n",
    "        # scatter = torch_scatter.scatter_add(exps, index.view(-1), dim=dim)\n",
    "        \n",
    "        # scatter = scatter_add(exps, index.view(-1), dim=dim)\n",
    "        \n",
    "        scatter_res = scatter(exps, index, dim=dim, reduce='sum')\n",
    "        \n",
    "        # size = torch.tensor(exps.size())\n",
    "        # size[dim] = index.max() + 1\n",
    "        # scatter = torch.zeros(*size, dtype=exps.dtype, device=exps.device)\n",
    "        # index_expand = broadcast(index, exps, dim)\n",
    "        # scatter.scatter_add_(dim, index_expand, exps)\n",
    "                \n",
    "        scatter_res = torch.clamp(scatter_res, eps)\n",
    "\n",
    "        sqrt = torch.pow(scatter_res, torch.relu(self.weights[4:8]))\n",
    "        alpha_beta = self.weights[8:12].view(1,1,4,-1)\n",
    "        terms = sqrt * alpha_beta\n",
    "\n",
    "        num = torch.sum(terms * self.num_idx, dim=2)\n",
    "        den = torch.sum(terms * self.den_idx, dim=2)\n",
    "        \n",
    "        multiplier = 2.0*torch.clamp(torch.sign(den), min=0.0) - 1.0\n",
    "\n",
    "        den = torch.where((den < eps) & (den > -eps), multiplier*eps, den)\n",
    "\n",
    "        res = num / den\n",
    "        return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GINConv\n",
    "from torch.nn import Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAF Aggregation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"laf.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLAFConv(GINConv):\n",
    "    def __init__(self, nn, units=1, node_dim=32, **kwargs):\n",
    "        super(GINLAFConv, self).__init__(nn, **kwargs)\n",
    "        self.laf = LAFLayer(units=units, kernel_initializer='random_uniform')\n",
    "        self.mlp = torch.nn.Linear(node_dim*units, node_dim)\n",
    "        self.dim = node_dim\n",
    "        self.units = units\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        x = torch.sigmoid(inputs)\n",
    "        x = self.laf(x, index)\n",
    "        x = x.view((-1, self.dim * self.units))\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PNA Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pna.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scatter_raw import (\n",
    "    scatter_add_raw as scatter_add,\n",
    "    scatter_max_raw as scatter_max,\n",
    "    scatter_mean_raw as scatter_mean\n",
    ")\n",
    "\n",
    "class GINPNAConv(GINConv):\n",
    "    def __init__(self, nn, node_dim=32, **kwargs):\n",
    "        super(GINPNAConv, self).__init__(nn, **kwargs)\n",
    "        self.mlp = torch.nn.Linear(node_dim*12, node_dim)\n",
    "        self.delta = 2.5749\n",
    "    \n",
    "    def aggregate(self, inputs, index):\n",
    "        sums = scatter_add(inputs, index, dim=0)\n",
    "        maxs = scatter_max(inputs, index, dim=0)[0]\n",
    "        means = scatter_mean(inputs, index, dim=0)\n",
    "        var = torch.relu(scatter_mean(inputs ** 2, index, dim=0) - means ** 2)\n",
    "        \n",
    "        aggrs = [sums, maxs, means, var]\n",
    "        c_idx = index.bincount().float().view(-1, 1)\n",
    "        l_idx = torch.log(c_idx + 1.)\n",
    "        \n",
    "        amplification_scaler = [c_idx / self.delta * a for a in aggrs]\n",
    "        attenuation_scaler = [self.delta / c_idx * a for a in aggrs]\n",
    "        combinations = torch.cat(aggrs+ amplification_scaler+ attenuation_scaler, dim=1)\n",
    "        x = self.mlp(combinations)\n",
    "    \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the new classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing, SAGEConv, GINConv, global_add_pool\n",
    "# import torch_scatter\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "import os.path as osp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "path = osp.join('./', 'data', 'TU')\n",
    "dataset = TUDataset(path, name='MUTAG').shuffle()\n",
    "test_dataset = dataset[:len(dataset) // 10]\n",
    "train_dataset = dataset[len(dataset) // 10:]\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAFNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LAFNet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "        units = 3\n",
    "        \n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINLAFConv(nn1, units=units, node_dim=num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINLAFConv(nn2, units=units, node_dim=dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINLAFConv(nn3, units=units, node_dim=dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINLAFConv(nn4, units=units, node_dim=dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINLAFConv(nn5, units=units, node_dim=dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNANet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "\n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINPNAConv(nn1, node_dim=num_features)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINPNAConv(nn2, node_dim=dim)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINPNAConv(nn3, node_dim=dim)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINPNAConv(nn4, node_dim=dim)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINPNAConv(nn5, node_dim=dim)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GINNet, self).__init__()\n",
    "\n",
    "        num_features = dataset.num_features\n",
    "        dim = 32\n",
    "\n",
    "        nn1 = Sequential(Linear(num_features, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn2 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv2 = GINConv(nn2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn3 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv3 = GINConv(nn3)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn4 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv4 = GINConv(nn4)\n",
    "        self.bn4 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        nn5 = Sequential(Linear(dim, dim), ReLU(), Linear(dim, dim))\n",
    "        self.conv5 = GINConv(nn5)\n",
    "        self.bn5 = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        self.fc1 = Linear(dim, dim)\n",
    "        self.fc2 = Linear(dim, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(self.conv5(x, edge_index))\n",
    "        x = self.bn5(x)\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 2113407800 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623b32077638475c8037e346b313d0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:768: UserWarning: aten::dropout: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /npu-stack/pytorch-fork/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 0.6767878, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 002, Train Loss: 0.6403571, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 003, Train Loss: 0.6012708, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 004, Train Loss: 0.5372186, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 005, Train Loss: 0.5025424, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 006, Train Loss: 0.4828761, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 007, Train Loss: 0.5421366, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 008, Train Loss: 0.4701187, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 009, Train Loss: 0.4110168, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 010, Train Loss: 0.4296606, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 011, Train Loss: 0.3704914, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 012, Train Loss: 0.3365514, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 013, Train Loss: 0.4190622, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 014, Train Loss: 0.4236781, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 015, Train Loss: 0.3739513, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 016, Train Loss: 0.3199419, Train Acc: 0.6705882, Test Acc: 0.6111111\n",
      "Epoch: 017, Train Loss: 0.3044021, Train Acc: 0.7058824, Test Acc: 0.9444444\n",
      "Epoch: 018, Train Loss: 0.3236228, Train Acc: 0.7117647, Test Acc: 0.8888889\n",
      "Epoch: 019, Train Loss: 0.3303434, Train Acc: 0.7705882, Test Acc: 0.8333333\n",
      "Epoch: 020, Train Loss: 0.3546370, Train Acc: 0.7882353, Test Acc: 0.8333333\n",
      "Epoch: 021, Train Loss: 0.3359150, Train Acc: 0.7411765, Test Acc: 0.8888889\n",
      "Epoch: 022, Train Loss: 0.2911397, Train Acc: 0.7294118, Test Acc: 0.9444444\n",
      "Epoch: 023, Train Loss: 0.3068025, Train Acc: 0.7294118, Test Acc: 0.9444444\n",
      "Epoch: 024, Train Loss: 0.3459045, Train Acc: 0.7294118, Test Acc: 0.8888889\n",
      "Epoch: 025, Train Loss: 0.3491835, Train Acc: 0.7352941, Test Acc: 0.8333333\n",
      "Epoch: 026, Train Loss: 0.3174858, Train Acc: 0.8470588, Test Acc: 0.8333333\n",
      "Epoch: 027, Train Loss: 0.2898843, Train Acc: 0.6588235, Test Acc: 0.6111111\n",
      "Epoch: 028, Train Loss: 0.3012000, Train Acc: 0.5529412, Test Acc: 0.5555556\n",
      "Epoch: 029, Train Loss: 0.2929822, Train Acc: 0.5882353, Test Acc: 0.6111111\n",
      "Epoch: 030, Train Loss: 0.2779822, Train Acc: 0.5470588, Test Acc: 0.5555556\n",
      "Epoch: 031, Train Loss: 0.2835307, Train Acc: 0.5000000, Test Acc: 0.5000000\n",
      "Epoch: 032, Train Loss: 0.2901583, Train Acc: 0.7941176, Test Acc: 0.7777778\n",
      "Epoch: 033, Train Loss: 0.2913738, Train Acc: 0.7823529, Test Acc: 0.8888889\n",
      "Epoch: 034, Train Loss: 0.2851719, Train Acc: 0.7705882, Test Acc: 0.9444444\n",
      "Epoch: 035, Train Loss: 0.2838155, Train Acc: 0.8294118, Test Acc: 0.8333333\n",
      "Epoch: 036, Train Loss: 0.2355790, Train Acc: 0.4176471, Test Acc: 0.3888889\n",
      "Epoch: 037, Train Loss: 0.2428503, Train Acc: 0.8764706, Test Acc: 0.8888889\n",
      "Epoch: 038, Train Loss: 0.2369371, Train Acc: 0.7470588, Test Acc: 0.8333333\n",
      "Epoch: 039, Train Loss: 0.2662073, Train Acc: 0.7647059, Test Acc: 0.9444444\n",
      "Epoch: 040, Train Loss: 0.2307669, Train Acc: 0.8705882, Test Acc: 0.8888889\n",
      "Epoch: 041, Train Loss: 0.2510141, Train Acc: 0.8117647, Test Acc: 0.8333333\n",
      "Epoch: 042, Train Loss: 0.2385902, Train Acc: 0.7705882, Test Acc: 0.7777778\n",
      "Epoch: 043, Train Loss: 0.2467730, Train Acc: 0.5058824, Test Acc: 0.5000000\n",
      "Epoch: 044, Train Loss: 0.2333636, Train Acc: 0.3352941, Test Acc: 0.3888889\n",
      "Epoch: 045, Train Loss: 0.2271382, Train Acc: 0.5294118, Test Acc: 0.6111111\n",
      "Epoch: 046, Train Loss: 0.2386230, Train Acc: 0.8000000, Test Acc: 1.0000000\n",
      "Epoch: 047, Train Loss: 0.2454966, Train Acc: 0.7588235, Test Acc: 0.8333333\n",
      "Epoch: 048, Train Loss: 0.2645965, Train Acc: 0.8235294, Test Acc: 0.8888889\n",
      "Epoch: 049, Train Loss: 0.2481508, Train Acc: 0.8941176, Test Acc: 0.7777778\n",
      "Epoch: 050, Train Loss: 0.2080349, Train Acc: 0.8647059, Test Acc: 0.8888889\n",
      "Epoch: 051, Train Loss: 0.2599844, Train Acc: 0.8117647, Test Acc: 0.8333333\n",
      "Epoch: 052, Train Loss: 0.2470482, Train Acc: 0.7882353, Test Acc: 0.8333333\n",
      "Epoch: 053, Train Loss: 0.2355846, Train Acc: 0.8823529, Test Acc: 0.8333333\n",
      "Epoch: 054, Train Loss: 0.2267500, Train Acc: 0.8058824, Test Acc: 0.8333333\n",
      "Epoch: 055, Train Loss: 0.2321288, Train Acc: 0.7235294, Test Acc: 0.7222222\n",
      "Epoch: 056, Train Loss: 0.2141726, Train Acc: 0.7705882, Test Acc: 0.7777778\n",
      "Epoch: 057, Train Loss: 0.2655888, Train Acc: 0.8941176, Test Acc: 0.8333333\n",
      "Epoch: 058, Train Loss: 0.2292541, Train Acc: 0.9000000, Test Acc: 0.8333333\n",
      "Epoch: 059, Train Loss: 0.2149618, Train Acc: 0.8294118, Test Acc: 0.8333333\n",
      "Epoch: 060, Train Loss: 0.2245991, Train Acc: 0.8176471, Test Acc: 0.8333333\n",
      "Epoch: 061, Train Loss: 0.2134473, Train Acc: 0.9117647, Test Acc: 0.8888889\n",
      "Epoch: 062, Train Loss: 0.2137670, Train Acc: 0.9176471, Test Acc: 0.8888889\n",
      "Epoch: 063, Train Loss: 0.2140950, Train Acc: 0.8823529, Test Acc: 0.8888889\n",
      "Epoch: 064, Train Loss: 0.2224779, Train Acc: 0.9294118, Test Acc: 0.8333333\n",
      "Epoch: 065, Train Loss: 0.2082197, Train Acc: 0.9117647, Test Acc: 0.8333333\n",
      "Epoch: 066, Train Loss: 0.1989404, Train Acc: 0.8823529, Test Acc: 0.8888889\n",
      "Epoch: 067, Train Loss: 0.1874257, Train Acc: 0.8588235, Test Acc: 0.8888889\n",
      "Epoch: 068, Train Loss: 0.2503780, Train Acc: 0.8470588, Test Acc: 0.8888889\n",
      "Epoch: 069, Train Loss: 0.2236472, Train Acc: 0.9176471, Test Acc: 0.8333333\n",
      "Epoch: 070, Train Loss: 0.2233444, Train Acc: 0.7941176, Test Acc: 0.7777778\n",
      "Epoch: 071, Train Loss: 0.2066813, Train Acc: 0.6529412, Test Acc: 0.7222222\n",
      "Epoch: 072, Train Loss: 0.2076329, Train Acc: 0.8470588, Test Acc: 0.6666667\n",
      "Epoch: 073, Train Loss: 0.2096636, Train Acc: 0.7941176, Test Acc: 0.8333333\n",
      "Epoch: 074, Train Loss: 0.2249484, Train Acc: 0.7882353, Test Acc: 0.8333333\n",
      "Epoch: 075, Train Loss: 0.2237178, Train Acc: 0.9117647, Test Acc: 0.7777778\n",
      "Epoch: 076, Train Loss: 0.2235544, Train Acc: 0.7529412, Test Acc: 0.6666667\n",
      "Epoch: 077, Train Loss: 0.2347646, Train Acc: 0.8000000, Test Acc: 0.6666667\n",
      "Epoch: 078, Train Loss: 0.1786965, Train Acc: 0.9176471, Test Acc: 0.7777778\n",
      "Epoch: 079, Train Loss: 0.1899907, Train Acc: 0.9235294, Test Acc: 0.7777778\n",
      "Epoch: 080, Train Loss: 0.1918317, Train Acc: 0.8647059, Test Acc: 0.7222222\n",
      "Epoch: 081, Train Loss: 0.2010743, Train Acc: 0.8176471, Test Acc: 0.7222222\n",
      "Epoch: 082, Train Loss: 0.2142036, Train Acc: 0.8176471, Test Acc: 0.7222222\n",
      "Epoch: 083, Train Loss: 0.1842753, Train Acc: 0.8176471, Test Acc: 0.7222222\n",
      "Epoch: 084, Train Loss: 0.2072025, Train Acc: 0.8117647, Test Acc: 0.6111111\n",
      "Epoch: 085, Train Loss: 0.2242635, Train Acc: 0.8117647, Test Acc: 0.7222222\n",
      "Epoch: 086, Train Loss: 0.1988804, Train Acc: 0.8000000, Test Acc: 0.7222222\n",
      "Epoch: 087, Train Loss: 0.2135334, Train Acc: 0.8117647, Test Acc: 0.7777778\n",
      "Epoch: 088, Train Loss: 0.2405552, Train Acc: 0.9176471, Test Acc: 0.7777778\n",
      "Epoch: 089, Train Loss: 0.1873274, Train Acc: 0.8705882, Test Acc: 0.8333333\n",
      "Epoch: 090, Train Loss: 0.2180274, Train Acc: 0.8470588, Test Acc: 0.8333333\n",
      "Epoch: 091, Train Loss: 0.1870082, Train Acc: 0.8588235, Test Acc: 0.7777778\n",
      "Epoch: 092, Train Loss: 0.1995669, Train Acc: 0.8705882, Test Acc: 0.8333333\n",
      "Epoch: 093, Train Loss: 0.1992656, Train Acc: 0.8058824, Test Acc: 0.8333333\n",
      "Epoch: 094, Train Loss: 0.1913753, Train Acc: 0.7882353, Test Acc: 0.8333333\n",
      "Epoch: 095, Train Loss: 0.2191297, Train Acc: 0.8235294, Test Acc: 0.8333333\n",
      "Epoch: 096, Train Loss: 0.1978272, Train Acc: 0.9000000, Test Acc: 0.8888889\n",
      "Epoch: 097, Train Loss: 0.1874136, Train Acc: 0.8176471, Test Acc: 0.7777778\n",
      "Epoch: 098, Train Loss: 0.1862658, Train Acc: 0.6352941, Test Acc: 0.7222222\n",
      "Epoch: 099, Train Loss: 0.1928843, Train Acc: 0.4882353, Test Acc: 0.5555556\n",
      "Epoch: 100, Train Loss: 0.2021294, Train Acc: 0.5705882, Test Acc: 0.6666667\n"
     ]
    }
   ],
   "source": [
    "from rich import traceback\n",
    "traceback.install()\n",
    "\n",
    "from tqdm.auto import trange\n",
    "\n",
    "# device = torch.device(\"hpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "net = \"LAF\"\n",
    "if net == \"LAF\":\n",
    "    model = LAFNet().to(device)\n",
    "elif net == \"PNA\":\n",
    "    model = PNANet().to(device)\n",
    "elif net == \"GIN\":\n",
    "    model = GINNet().to(device)\n",
    "\n",
    "def train(epoch):\n",
    "    if epoch == 51:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 0.5 * param_group['lr']\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(output, data.y)        \n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "def test(loader):    \n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device_cpu)\n",
    "        output = model(data.x, data.edge_index, data.batch)\n",
    "        pred = output.max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()    \n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "model.train()\n",
    "# model = torch.compile(model, backend=\"hpu_backend\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in trange(1, 101):\n",
    "    train_loss = train(epoch)\n",
    "    model.to(device_cpu)\n",
    "    with torch.inference_mode():\n",
    "        model.eval()\n",
    "        train_acc = test(train_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        print('Epoch: {:03d}, Train Loss: {:.7f}, '\n",
    "            'Train Acc: {:.7f}, Test Acc: {:.7f}'.format(epoch, train_loss,\n",
    "                                                        train_acc, test_acc))\n",
    "    model.to(device)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
