<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800000; text-decoration-color: #800000">╭─────────────────────────────── </span><span style="color: #800000; text-decoration-color: #800000; font-weight: bold">Traceback </span><span style="color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold">(most recent call last)</span><span style="color: #800000; text-decoration-color: #800000"> ────────────────────────────────╮</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> in <span style="color: #00ff00; text-decoration-color: #00ff00">&lt;module&gt;</span>:<span style="color: #0000ff; text-decoration-color: #0000ff">4</span>                                                                                    <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">1 </span>model.train()                                                                                <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">2 </span>model = torch.compile(model, backend=<span style="color: #808000; text-decoration-color: #808000">"hpu_backend"</span>)                                          <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">3 </span><span style="color: #0000ff; text-decoration-color: #0000ff">for</span> epoch <span style="color: #ff00ff; text-decoration-color: #ff00ff">in</span> <span style="color: #00ffff; text-decoration-color: #00ffff">range</span>(<span style="color: #0000ff; text-decoration-color: #0000ff">1</span>, <span style="color: #0000ff; text-decoration-color: #0000ff">2</span>):                                                                    <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #800000; text-decoration-color: #800000">❱ </span>4 <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span>train(epoch)                                                                             <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">5 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span>acc = test()                                                                             <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">6 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #00ffff; text-decoration-color: #00ffff">print</span>(<span style="color: #808000; text-decoration-color: #808000">f'Epoch: {</span>epoch<span style="color: #808000; text-decoration-color: #808000">}, Accuracy: {</span>acc<span style="color: #808000; text-decoration-color: #808000">:.4f}'</span>)                                            <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">7 </span>                                                                                             <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> in <span style="color: #00ff00; text-decoration-color: #00ff00">train</span>:<span style="color: #0000ff; text-decoration-color: #0000ff">8</span>                                                                                       <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 5 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #0000ff; text-decoration-color: #0000ff">for</span> i, (pos_rw, neg_rw) <span style="color: #ff00ff; text-decoration-color: #ff00ff">in</span> <span style="color: #00ffff; text-decoration-color: #00ffff">enumerate</span>(loader):                                           <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 6 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>optimizer.zero_grad()                                                               <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 7 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>loss = model.loss(pos_rw.to(device), neg_rw.to(device))                             <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #800000; text-decoration-color: #800000">❱ </span> 8 <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>loss.backward()                                                                     <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 9 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>optimizer.step()                                                                    <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">10 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>                                                                                    <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">11 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>total_loss += loss.item()                                                           <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #bfbf7f; text-decoration-color: #bfbf7f">/usr/local/lib/python3.10/dist-packages/torch/</span><span style="color: #808000; text-decoration-color: #808000; font-weight: bold">_tensor.py</span>:<span style="color: #0000ff; text-decoration-color: #0000ff">531</span> in <span style="color: #00ff00; text-decoration-color: #00ff00">backward</span>                         <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 528 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   │   │   </span>create_graph=create_graph,                                                <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 529 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   │   │   </span>inputs=inputs,                                                            <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 530 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   │   </span>)                                                                             <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #800000; text-decoration-color: #800000">❱ </span> 531 <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>torch.autograd.backward(                                                          <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 532 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   │   </span><span style="color: #00ffff; text-decoration-color: #00ffff">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 533 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>)                                                                                 <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"> 534 </span>                                                                                          <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #bfbf7f; text-decoration-color: #bfbf7f">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style="color: #808000; text-decoration-color: #808000; font-weight: bold">__init__.py</span>:<span style="color: #0000ff; text-decoration-color: #0000ff">289</span> in <span style="color: #00ff00; text-decoration-color: #00ff00">backward</span>               <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">286 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f"># The reason we repeat the same comment below is that</span>                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">287 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f"># some Python versions print out the first line of a multi-line function</span>               <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">288 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f"># calls in the traceback and some print out the last line</span>                              <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #800000; text-decoration-color: #800000">❱ </span>289 <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span>_engine_run_backward(                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">290 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>tensors,                                                                           <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">291 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>grad_tensors_,                                                                     <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">292 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>retain_graph,                                                                      <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #bfbf7f; text-decoration-color: #bfbf7f">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style="color: #808000; text-decoration-color: #808000; font-weight: bold">graph.py</span>:<span style="color: #0000ff; text-decoration-color: #0000ff">768</span> in <span style="color: #00ff00; text-decoration-color: #00ff00">_engine_run_backward</span>      <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>                                                                                                  <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">765 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #0000ff; text-decoration-color: #0000ff">if</span> attach_logging_hooks:                                                               <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">766 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)               <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">767 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #0000ff; text-decoration-color: #0000ff">try</span>:                                                                                   <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span> <span style="color: #800000; text-decoration-color: #800000">❱ </span>768 <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span><span style="color: #0000ff; text-decoration-color: #0000ff">return</span> Variable._execution_engine.run_backward(  <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"># Calls into the C++ engine to </span>   <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">769 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   │   </span>t_outputs, *args, **kwargs                                                     <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">770 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   │   </span>)  <span style="color: #7f7f7f; text-decoration-color: #7f7f7f"># Calls into the C++ engine to run the backward pass</span>                            <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">│</span>   <span style="color: #7f7f7f; text-decoration-color: #7f7f7f">771 </span><span style="color: #7f7f7f; text-decoration-color: #7f7f7f">│   </span><span style="color: #0000ff; text-decoration-color: #0000ff">finally</span>:                                                                               <span style="color: #800000; text-decoration-color: #800000">│</span>
<span style="color: #800000; text-decoration-color: #800000">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>
<span style="color: #ff0000; text-decoration-color: #ff0000; font-weight: bold">NotImplementedError: </span>Could not run <span style="color: #008000; text-decoration-color: #008000">'aten::_sparse_coo_tensor_with_dims_and_tensors'</span> with arguments from the 
<span style="color: #008000; text-decoration-color: #008000">'SparseHPU'</span> backend. This could be because the operator doesn't exist for this backend, or was omitted during the 
selective/custom build process <span style="font-weight: bold">(</span>if using custom build<span style="font-weight: bold">)</span>. If you are a Facebook employee using PyTorch on mobile, 
please visit <span style="color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline">https://fburl.com/ptmfixes</span> for possible resolutions. <span style="color: #008000; text-decoration-color: #008000">'aten::_sparse_coo_tensor_with_dims_and_tensors'</span> 
is only available for these backends: <span style="font-weight: bold">[</span>HPU, Meta, SparseCPU, SparseMeta, BackendSelect, Python, 
FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, 
AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, 
AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3,
AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastHPU, AutocastCUDA, FuncTorchBatched, 
BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, 
FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher<span style="font-weight: bold">]</span>.

HPU: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-integration/hpu_ops/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">cpu_fallback.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">116</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
Meta: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">RegisterMeta.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">26993</span> <span style="font-weight: bold">[</span>kernel<span style="font-weight: bold">]</span>
SparseCPU: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">RegisterSparseCPU.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1387</span> <span style="font-weight: bold">[</span>kernel<span style="font-weight: bold">]</span>
SparseMeta: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">RegisterSparseMeta.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">287</span> <span style="font-weight: bold">[</span>kernel<span style="font-weight: bold">]</span>
BackendSelect: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">RegisterBackendSelect.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">815</span> <span style="font-weight: bold">[</span>kernel<span style="font-weight: bold">]</span>
Python: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">PythonFallbackKernel.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">153</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
FuncTorchDynamicLayerBackMode: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">DynamicLayer.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">497</span> 
<span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
Functionalize: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">FunctionalizeFallbackKernel.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">349</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
Named: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">NamedRegistrations.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">7</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
Conjugate: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">ConjugateFallback.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
Negative: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/native/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">NegateFallback.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">18</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
ZeroTensor: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">ZeroTensorFallback.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">86</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
ADInplaceOrView: fallthrough registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableFallbackKernel.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">86</span>
<span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
AutogradOther: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradCPU: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd
kernel<span style="font-weight: bold">]</span>
AutogradCUDA: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradHIP: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd
kernel<span style="font-weight: bold">]</span>
AutogradXLA: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd
kernel<span style="font-weight: bold">]</span>
AutogradMPS: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd
kernel<span style="font-weight: bold">]</span>
AutogradIPU: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd
kernel<span style="font-weight: bold">]</span>
AutogradXPU: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd
kernel<span style="font-weight: bold">]</span>
AutogradHPU: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd
kernel<span style="font-weight: bold">]</span>
AutogradVE: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> <span style="font-weight: bold">[</span>autograd 
kernel<span style="font-weight: bold">]</span>
AutogradLazy: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradMTIA: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradPrivateUse1: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradPrivateUse2: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradPrivateUse3: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradMeta: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
AutogradNestedTensor: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VariableType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">19857</span> 
<span style="font-weight: bold">[</span>autograd kernel<span style="font-weight: bold">]</span>
Tracer: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">TraceType_2.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">17623</span> <span style="font-weight: bold">[</span>kernel<span style="font-weight: bold">]</span>
AutocastCPU: fallthrough registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">autocast_mode.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">209</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
AutocastXPU: fallthrough registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">autocast_mode.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">351</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
AutocastHPU: fallthrough registered at 
<span style="color: #800080; text-decoration-color: #800080">/builds/pytorch_modules_multi_build/torch/py3.10/pt2.4.0/Release/generated/backend/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">hpu_autocast_ops0.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">49</span> 
<span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
AutocastCUDA: fallthrough registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">autocast_mode.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">165</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
FuncTorchBatched: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">LegacyBatchingRegistrations.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">731</span>
<span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
BatchedNestedTensor: registered at 
<span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">LegacyBatchingRegistrations.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">758</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
FuncTorchVmapMode: fallthrough registered at 
<span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VmapModeRegistrations.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">27</span> <span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
Batched: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">LegacyBatchingRegistrations.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1075</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
VmapMode: fallthrough registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">VmapModeRegistrations.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">33</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
FuncTorchGradWrapper: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">TensorWrapper.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">207</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
PythonTLSSnapshot: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">PythonFallbackKernel.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">161</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
FuncTorchDynamicLayerFrontMode: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">DynamicLayer.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">493</span> 
<span style="font-weight: bold">[</span>backend fallback<span style="font-weight: bold">]</span>
PreDispatch: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">PythonFallbackKernel.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">165</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>
PythonDispatcher: registered at <span style="color: #800080; text-decoration-color: #800080">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style="color: #ff00ff; text-decoration-color: #ff00ff">PythonFallbackKernel.cpp</span>:<span style="color: #008080; text-decoration-color: #008080; font-weight: bold">157</span> <span style="font-weight: bold">[</span>backend 
fallback<span style="font-weight: bold">]</span>

</pre>
