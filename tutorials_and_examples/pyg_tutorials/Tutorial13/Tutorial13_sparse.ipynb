{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT_HPU_LAZY_MODE: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling add_step_closure function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n",
      "Calling iter_mark_step function does not have any effect. It's lazy mode only functionality. (warning logged once)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0a0+git74cd574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/inspect.py:288: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(object, types.FunctionType)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the eager mode\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '0'\n",
    "\n",
    "# Verify the environment variable is set\n",
    "print(f\"PT_HPU_LAZY_MODE: {os.environ['PT_HPU_LAZY_MODE']}\")\n",
    "\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.datasets import AMiner\n",
    "from torch_geometric.nn import MetaPath2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method InteractiveShell.excepthook of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7fe7737707f0>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use rich traceback\n",
    "\n",
    "from rich import traceback\n",
    "traceback.install()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaPath2Vec\n",
    "\n",
    "[paper](https://ericdongyx.github.io/papers/KDD17-dong-chawla-swami-metapath2vec.pdf)  \n",
    "[code](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/metapath2vec.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"hpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "============================= HABANA PT BRIDGE CONFIGURATION =========================== \n",
      " PT_HPU_LAZY_MODE = 0\n",
      " PT_RECIPE_CACHE_PATH = \n",
      " PT_CACHE_FOLDER_DELETE = 0\n",
      " PT_HPU_RECIPE_CACHE_CONFIG = \n",
      " PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807\n",
      " PT_HPU_LAZY_ACC_PAR_MODE = 1\n",
      " PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 0\n",
      " PT_HPU_EAGER_PIPELINE_ENABLE = 1\n",
      " PT_HPU_EAGER_COLLECTIVE_PIPELINE_ENABLE = 1\n",
      "---------------------------: System Configuration :---------------------------\n",
      "Num CPU Cores : 160\n",
      "CPU RAM       : 2113407800 KB\n",
      "------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "path = osp.join('.', 'data', 'AMiner')\n",
    "dataset = AMiner(path)\n",
    "data = dataset[0]\n",
    "\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  author={\n",
      "    y=[246678],\n",
      "    y_index=[246678],\n",
      "    num_nodes=1693531,\n",
      "  },\n",
      "  venue={\n",
      "    y=[134],\n",
      "    y_index=[134],\n",
      "    num_nodes=3883,\n",
      "  },\n",
      "  paper={ num_nodes=3194405 },\n",
      "  (paper, written_by, author)={ edge_index=[2, 9323605] },\n",
      "  (author, writes, paper)={ edge_index=[2, 9323605] },\n",
      "  (paper, published_in, venue)={ edge_index=[2, 3194405] },\n",
      "  (venue, publishes, paper)={ edge_index=[2, 3194405] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tensor([[      0,       1,       2,  ..., 3194404, 3194404, 3194404],\n",
      "        [      0,       1,       2,  ...,    4393,   21681,  317436]],\n",
      "       device='hpu:0')\n"
     ]
    }
   ],
   "source": [
    "print(type(data.edge_index_dict))\n",
    "print(data.edge_index_dict[('paper', 'written_by', 'author')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('paper', 'written_by', 'author'), ('author', 'writes', 'paper'), ('paper', 'published_in', 'venue'), ('venue', 'publishes', 'paper')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'author': 1693531, 'venue': 3883, 'paper': 3194405}\n"
     ]
    }
   ],
   "source": [
    "print(type(data.num_nodes_dict))\n",
    "print(data.num_nodes_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7,\n",
      "        7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], device='hpu:0')\n"
     ]
    }
   ],
   "source": [
    "print(type(data.y_dict))\n",
    "print(data.y_dict[\"venue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "tensor([1741, 2245,  111,  837, 2588, 2116, 2696, 3648, 3784,  313, 3414,  598,\n",
      "        2995, 2716, 1423,  783, 1902, 3132, 1753, 2748, 2660, 3182,  775, 3339,\n",
      "        1601, 3589,  156, 1145,  692, 3048,  925, 1587,  820, 1374, 3719,  819,\n",
      "         492, 3830, 2777, 3001, 3693,  517, 1808, 2353, 3499, 1763, 2372, 1030,\n",
      "         721, 2680, 3355, 1217, 3400, 1271, 1970, 1127,  407,  353, 1471, 1095,\n",
      "         477, 3701,   65, 1009, 1899, 1442, 2073, 3143, 2466,  289, 1996, 1070,\n",
      "        3871, 3695,  281, 3633,   50, 2642, 1925, 1285, 2587, 3814, 3582, 1873,\n",
      "        1339, 3450,  271, 2966,  453, 2638, 1354, 3211,  391, 1588, 3875, 2216,\n",
      "        2146, 3765, 2486,  661, 3367,  426,  750, 2158,  519,  230, 1677,  839,\n",
      "        2945, 1313, 1037, 2879, 2225, 3523, 1247,  448,  227, 3385,  529, 2849,\n",
      "        1584, 1229,  373, 2235, 1819, 1764, 3155, 2852, 2789, 3474, 1571, 2088,\n",
      "         208,  462], device='hpu:0')\n"
     ]
    }
   ],
   "source": [
    "print(type(data.y_index_dict))\n",
    "print(data.y_index_dict[\"venue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "# dict_keys([('paper', 'written_by', 'author'), ('author', 'writes', 'paper'), ('paper', 'published_in', 'venue'), ('venue', 'publishes', 'paper')])\n",
    "\n",
    "metapath = [\n",
    "    ('author', 'writes', 'paper'),\n",
    "    ('paper', 'published_in', 'venue'),\n",
    "    ('venue', 'publishes', 'paper'),\n",
    "    ('paper', 'written_by', 'author'),\n",
    "]\n",
    "\n",
    "\n",
    "model = MetaPath2Vec(data.edge_index_dict, \n",
    "                     embedding_dim=128,\n",
    "                     metapath=metapath,\n",
    "                     walk_length=5, \n",
    "                     context_size=3,\n",
    "                     walks_per_node=3,\n",
    "                     num_negative_samples=1,\n",
    "                     sparse=True,\n",
    "                    #  sparse=False,\n",
    "                    ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the loader to build a loader\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "1 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "2 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "3 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "4 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "5 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "6 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "7 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "8 torch.Size([1536, 3]) torch.Size([1536, 3])\n",
      "9 torch.Size([1536, 3]) torch.Size([1536, 3])\n"
     ]
    }
   ],
   "source": [
    "for idx, (pos_rw, neg_rw) in enumerate(loader):\n",
    "    if idx == 10: break\n",
    "    print(idx, pos_rw.shape, neg_rw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  73110, 4891819, 4891819]) tensor([  73110, 3118844, 4888717])\n"
     ]
    }
   ],
   "source": [
    "print(pos_rw[0],neg_rw[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizialize optimizer\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "# optimizer = torch.optim.Adam(list(model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, log_steps=500, eval_steps=1000):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                   f'Loss: {total_loss / log_steps:.4f}'))\n",
    "            total_loss = 0\n",
    "\n",
    "        if (i + 1) % eval_steps == 0:\n",
    "            acc = test()\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                   f'Acc: {acc:.4f}'))\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(train_ratio=0.1):\n",
    "    model.eval()\n",
    "\n",
    "    z = model('author', batch=data.y_index_dict['author'])\n",
    "    y = data.y_dict['author']\n",
    "\n",
    "    perm = torch.randperm(z.size(0), device=z.device)\n",
    "    train_perm = perm[:int(z.size(0) * train_ratio)]\n",
    "    test_perm = perm[int(z.size(0) * train_ratio):]\n",
    "\n",
    "    return model.test(z[train_perm], y[train_perm], z[test_perm],\n",
    "                      y[test_perm], max_iter=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span>model.train()                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>model = torch.compile(model, backend=<span style=\"color: #808000; text-decoration-color: #808000\">\"hpu_backend\"</span>)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> epoch <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>):                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>4 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>train(epoch)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>acc = test()                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">f'Epoch: {</span>epoch<span style=\"color: #808000; text-decoration-color: #808000\">}, Accuracy: {</span>acc<span style=\"color: #808000; text-decoration-color: #808000\">:.4f}'</span>)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">8</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i, (pos_rw, neg_rw) <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(loader):                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.zero_grad()                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss = model.loss(pos_rw.to(device), neg_rw.to(device))                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 8 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>loss.backward()                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer.step()                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>total_loss += loss.item()                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_tensor.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">531</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 528 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>create_graph=create_graph,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 529 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>inputs=inputs,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 530 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 531 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>torch.autograd.backward(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 532 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, gradient, retain_graph, create_graph, inputs=inputs                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 533 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 534 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">__init__.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">289</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">backward</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The reason we repeat the same comment below is that</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># some Python versions print out the first line of a multi-line function</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># calls in the traceback and some print out the last line</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>289 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>_engine_run_backward(                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">290 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>tensors,                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">291 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>grad_tensors_,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">292 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retain_graph,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/autograd/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">graph.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">768</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_engine_run_backward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">765 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attach_logging_hooks:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">766 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">767 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>768 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> Variable._execution_engine.run_backward(  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to </span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">769 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>t_outputs, *args, **kwargs                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Calls into the C++ engine to run the backward pass</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">finally</span>:                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NotImplementedError: </span>Could not run <span style=\"color: #008000; text-decoration-color: #008000\">'aten::_sparse_coo_tensor_with_dims_and_tensors'</span> with arguments from the \n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'SparseHPU'</span> backend. This could be because the operator doesn't exist for this backend, or was omitted during the \n",
       "selective/custom build process <span style=\"font-weight: bold\">(</span>if using custom build<span style=\"font-weight: bold\">)</span>. If you are a Facebook employee using PyTorch on mobile, \n",
       "please visit <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://fburl.com/ptmfixes</span> for possible resolutions. <span style=\"color: #008000; text-decoration-color: #008000\">'aten::_sparse_coo_tensor_with_dims_and_tensors'</span> \n",
       "is only available for these backends: <span style=\"font-weight: bold\">[</span>HPU, Meta, SparseCPU, SparseMeta, BackendSelect, Python, \n",
       "FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, \n",
       "AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, \n",
       "AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3,\n",
       "AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastHPU, AutocastCUDA, FuncTorchBatched, \n",
       "BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, \n",
       "FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher<span style=\"font-weight: bold\">]</span>.\n",
       "\n",
       "HPU: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-integration/hpu_ops/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">cpu_fallback.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">116</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Meta: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">RegisterMeta.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26993</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "SparseCPU: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">RegisterSparseCPU.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1387</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "SparseMeta: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">RegisterSparseMeta.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">287</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "BackendSelect: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/build/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">RegisterBackendSelect.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">815</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "Python: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">PythonFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">153</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "FuncTorchDynamicLayerBackMode: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DynamicLayer.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">497</span> \n",
       "<span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Functionalize: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">FunctionalizeFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">349</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "Named: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">NamedRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Conjugate: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ConjugateFallback.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Negative: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/native/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">NegateFallback.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "ZeroTensor: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ZeroTensorFallback.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">86</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "ADInplaceOrView: fallthrough registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">86</span>\n",
       "<span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "AutogradOther: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradCPU: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd\n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradCUDA: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradHIP: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd\n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradXLA: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd\n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradMPS: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd\n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradIPU: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd\n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradXPU: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd\n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradHPU: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd\n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradVE: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> <span style=\"font-weight: bold\">[</span>autograd \n",
       "kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradLazy: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradMTIA: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradPrivateUse1: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradPrivateUse2: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradPrivateUse3: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradMeta: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutogradNestedTensor: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VariableType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19857</span> \n",
       "<span style=\"font-weight: bold\">[</span>autograd kernel<span style=\"font-weight: bold\">]</span>\n",
       "Tracer: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/torch/csrc/autograd/generated/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">TraceType_2.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17623</span> <span style=\"font-weight: bold\">[</span>kernel<span style=\"font-weight: bold\">]</span>\n",
       "AutocastCPU: fallthrough registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">autocast_mode.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">209</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "AutocastXPU: fallthrough registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">autocast_mode.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">351</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "AutocastHPU: fallthrough registered at \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/builds/pytorch_modules_multi_build/torch/py3.10/pt2.4.0/Release/generated/backend/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">hpu_autocast_ops0.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span> \n",
       "<span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "AutocastCUDA: fallthrough registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">autocast_mode.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">165</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "FuncTorchBatched: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LegacyBatchingRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">731</span>\n",
       "<span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "BatchedNestedTensor: registered at \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LegacyBatchingRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">758</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "FuncTorchVmapMode: fallthrough registered at \n",
       "<span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VmapModeRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span> <span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "Batched: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">LegacyBatchingRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1075</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "VmapMode: fallthrough registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">VmapModeRegistrations.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">33</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "FuncTorchGradWrapper: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">TensorWrapper.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">207</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "PythonTLSSnapshot: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">PythonFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">161</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "FuncTorchDynamicLayerFrontMode: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/functorch/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">DynamicLayer.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">493</span> \n",
       "<span style=\"font-weight: bold\">[</span>backend fallback<span style=\"font-weight: bold\">]</span>\n",
       "PreDispatch: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">PythonFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">165</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "PythonDispatcher: registered at <span style=\"color: #800080; text-decoration-color: #800080\">/npu-stack/pytorch-fork/aten/src/ATen/core/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">PythonFallbackKernel.cpp</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">157</span> <span style=\"font-weight: bold\">[</span>backend \n",
       "fallback<span style=\"font-weight: bold\">]</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m4\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0mmodel.train()                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mmodel = torch.compile(model, backend=\u001b[33m\"\u001b[0m\u001b[33mhpu_backend\u001b[0m\u001b[33m\"\u001b[0m)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m\u001b[94mfor\u001b[0m epoch \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(\u001b[94m1\u001b[0m, \u001b[94m2\u001b[0m):                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m4 \u001b[2m│   \u001b[0mtrain(epoch)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[2m│   \u001b[0macc = test()                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mEpoch: \u001b[0m\u001b[33m{\u001b[0mepoch\u001b[33m}\u001b[0m\u001b[33m, Accuracy: \u001b[0m\u001b[33m{\u001b[0macc\u001b[33m:\u001b[0m\u001b[33m.4f\u001b[0m\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtrain\u001b[0m:\u001b[94m8\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m i, (pos_rw, neg_rw) \u001b[95min\u001b[0m \u001b[96menumerate\u001b[0m(loader):                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.zero_grad()                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   │   \u001b[0mloss = model.loss(pos_rw.to(device), neg_rw.to(device))                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 8 \u001b[2m│   │   \u001b[0mloss.backward()                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer.step()                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   │   \u001b[0mtotal_loss += loss.item()                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_tensor.py\u001b[0m:\u001b[94m531\u001b[0m in \u001b[92mbackward\u001b[0m                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 528 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mcreate_graph=create_graph,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 529 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs=inputs,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 530 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 531 \u001b[2m│   │   \u001b[0mtorch.autograd.backward(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 532 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m, gradient, retain_graph, create_graph, inputs=inputs                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 533 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 534 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m289\u001b[0m in \u001b[92mbackward\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# The reason we repeat the same comment below is that\u001b[0m                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# some Python versions print out the first line of a multi-line function\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# calls in the traceback and some print out the last line\u001b[0m                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m289 \u001b[2m│   \u001b[0m_engine_run_backward(                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m290 \u001b[0m\u001b[2m│   │   \u001b[0mtensors,                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m291 \u001b[0m\u001b[2m│   │   \u001b[0mgrad_tensors_,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m292 \u001b[0m\u001b[2m│   │   \u001b[0mretain_graph,                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/autograd/\u001b[0m\u001b[1;33mgraph.py\u001b[0m:\u001b[94m768\u001b[0m in \u001b[92m_engine_run_backward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m765 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m attach_logging_hooks:                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m766 \u001b[0m\u001b[2m│   │   \u001b[0munregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m767 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m768 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m Variable._execution_engine.run_backward(  \u001b[2m# Calls into the C++ engine to \u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m769 \u001b[0m\u001b[2m│   │   │   \u001b[0mt_outputs, *args, **kwargs                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m770 \u001b[0m\u001b[2m│   │   \u001b[0m)  \u001b[2m# Calls into the C++ engine to run the backward pass\u001b[0m                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m771 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfinally\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNotImplementedError: \u001b[0mCould not run \u001b[32m'aten::_sparse_coo_tensor_with_dims_and_tensors'\u001b[0m with arguments from the \n",
       "\u001b[32m'SparseHPU'\u001b[0m backend. This could be because the operator doesn't exist for this backend, or was omitted during the \n",
       "selective/custom build process \u001b[1m(\u001b[0mif using custom build\u001b[1m)\u001b[0m. If you are a Facebook employee using PyTorch on mobile, \n",
       "please visit \u001b[4;94mhttps://fburl.com/ptmfixes\u001b[0m for possible resolutions. \u001b[32m'aten::_sparse_coo_tensor_with_dims_and_tensors'\u001b[0m \n",
       "is only available for these backends: \u001b[1m[\u001b[0mHPU, Meta, SparseCPU, SparseMeta, BackendSelect, Python, \n",
       "FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, \n",
       "AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, \n",
       "AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3,\n",
       "AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastHPU, AutocastCUDA, FuncTorchBatched, \n",
       "BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, \n",
       "FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher\u001b[1m]\u001b[0m.\n",
       "\n",
       "HPU: registered at \u001b[35m/npu-stack/pytorch-integration/hpu_ops/\u001b[0m\u001b[95mcpu_fallback.cpp\u001b[0m:\u001b[1;36m116\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Meta: registered at \u001b[35m/npu-stack/pytorch-fork/build/aten/src/ATen/\u001b[0m\u001b[95mRegisterMeta.cpp\u001b[0m:\u001b[1;36m26993\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "SparseCPU: registered at \u001b[35m/npu-stack/pytorch-fork/build/aten/src/ATen/\u001b[0m\u001b[95mRegisterSparseCPU.cpp\u001b[0m:\u001b[1;36m1387\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "SparseMeta: registered at \u001b[35m/npu-stack/pytorch-fork/build/aten/src/ATen/\u001b[0m\u001b[95mRegisterSparseMeta.cpp\u001b[0m:\u001b[1;36m287\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "BackendSelect: registered at \u001b[35m/npu-stack/pytorch-fork/build/aten/src/ATen/\u001b[0m\u001b[95mRegisterBackendSelect.cpp\u001b[0m:\u001b[1;36m815\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "Python: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/core/\u001b[0m\u001b[95mPythonFallbackKernel.cpp\u001b[0m:\u001b[1;36m153\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "FuncTorchDynamicLayerBackMode: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/functorch/\u001b[0m\u001b[95mDynamicLayer.cpp\u001b[0m:\u001b[1;36m497\u001b[0m \n",
       "\u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Functionalize: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mFunctionalizeFallbackKernel.cpp\u001b[0m:\u001b[1;36m349\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "Named: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/core/\u001b[0m\u001b[95mNamedRegistrations.cpp\u001b[0m:\u001b[1;36m7\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Conjugate: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mConjugateFallback.cpp\u001b[0m:\u001b[1;36m17\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Negative: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/native/\u001b[0m\u001b[95mNegateFallback.cpp\u001b[0m:\u001b[1;36m18\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "ZeroTensor: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mZeroTensorFallback.cpp\u001b[0m:\u001b[1;36m86\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "ADInplaceOrView: fallthrough registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/core/\u001b[0m\u001b[95mVariableFallbackKernel.cpp\u001b[0m:\u001b[1;36m86\u001b[0m\n",
       "\u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "AutogradOther: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradCPU: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd\n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradCUDA: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradHIP: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd\n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradXLA: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd\n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradMPS: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd\n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradIPU: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd\n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradXPU: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd\n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradHPU: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd\n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradVE: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \u001b[1m[\u001b[0mautograd \n",
       "kernel\u001b[1m]\u001b[0m\n",
       "AutogradLazy: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradMTIA: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradPrivateUse1: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradPrivateUse2: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradPrivateUse3: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradMeta: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "AutogradNestedTensor: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mVariableType_2.cpp\u001b[0m:\u001b[1;36m19857\u001b[0m \n",
       "\u001b[1m[\u001b[0mautograd kernel\u001b[1m]\u001b[0m\n",
       "Tracer: registered at \u001b[35m/npu-stack/pytorch-fork/torch/csrc/autograd/generated/\u001b[0m\u001b[95mTraceType_2.cpp\u001b[0m:\u001b[1;36m17623\u001b[0m \u001b[1m[\u001b[0mkernel\u001b[1m]\u001b[0m\n",
       "AutocastCPU: fallthrough registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mautocast_mode.cpp\u001b[0m:\u001b[1;36m209\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "AutocastXPU: fallthrough registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mautocast_mode.cpp\u001b[0m:\u001b[1;36m351\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "AutocastHPU: fallthrough registered at \n",
       "\u001b[35m/builds/pytorch_modules_multi_build/torch/py3.10/pt2.4.0/Release/generated/backend/\u001b[0m\u001b[95mhpu_autocast_ops0.cpp\u001b[0m:\u001b[1;36m49\u001b[0m \n",
       "\u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "AutocastCUDA: fallthrough registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mautocast_mode.cpp\u001b[0m:\u001b[1;36m165\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "FuncTorchBatched: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/functorch/\u001b[0m\u001b[95mLegacyBatchingRegistrations.cpp\u001b[0m:\u001b[1;36m731\u001b[0m\n",
       "\u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "BatchedNestedTensor: registered at \n",
       "\u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/functorch/\u001b[0m\u001b[95mLegacyBatchingRegistrations.cpp\u001b[0m:\u001b[1;36m758\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "FuncTorchVmapMode: fallthrough registered at \n",
       "\u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/functorch/\u001b[0m\u001b[95mVmapModeRegistrations.cpp\u001b[0m:\u001b[1;36m27\u001b[0m \u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "Batched: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mLegacyBatchingRegistrations.cpp\u001b[0m:\u001b[1;36m1075\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "VmapMode: fallthrough registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/\u001b[0m\u001b[95mVmapModeRegistrations.cpp\u001b[0m:\u001b[1;36m33\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "FuncTorchGradWrapper: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/functorch/\u001b[0m\u001b[95mTensorWrapper.cpp\u001b[0m:\u001b[1;36m207\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "PythonTLSSnapshot: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/core/\u001b[0m\u001b[95mPythonFallbackKernel.cpp\u001b[0m:\u001b[1;36m161\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "FuncTorchDynamicLayerFrontMode: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/functorch/\u001b[0m\u001b[95mDynamicLayer.cpp\u001b[0m:\u001b[1;36m493\u001b[0m \n",
       "\u001b[1m[\u001b[0mbackend fallback\u001b[1m]\u001b[0m\n",
       "PreDispatch: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/core/\u001b[0m\u001b[95mPythonFallbackKernel.cpp\u001b[0m:\u001b[1;36m165\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "PythonDispatcher: registered at \u001b[35m/npu-stack/pytorch-fork/aten/src/ATen/core/\u001b[0m\u001b[95mPythonFallbackKernel.cpp\u001b[0m:\u001b[1;36m157\u001b[0m \u001b[1m[\u001b[0mbackend \n",
       "fallback\u001b[1m]\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "model = torch.compile(model, backend=\"hpu_backend\")\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch)\n",
    "    acc = test()\n",
    "    print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = MetaPath2Vec(data.edge_index_dict, \n",
    "                     embedding_dim=128,\n",
    "                     metapath=metapath,\n",
    "                     walk_length=5, \n",
    "                     context_size=3,\n",
    "                     walks_per_node=3,\n",
    "                     num_negative_samples=1,\n",
    "                     sparse=True\n",
    "                    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.embedding.weight[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "loaded_model.load_state_dict(torch.load(\"mymodel\").detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the model to cpu\n",
    "file = torch.load('mymodel', map_location=lambda storage, loc: storage)\n",
    "loaded_model.load_state_dict(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.embedding.weight[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_venue = loaded_model('venue', batch=data.y_index_dict['venue']).detach().numpy()\n",
    "z_auth = loaded_model('author', batch=data.y_index_dict['author']).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_venue = z_venue[0:100]\n",
    "z_auth = z_auth[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "embedder = umap.UMAP().fit(data,y)\n",
    "\n",
    "z_venue_2d = umap.UMAP().fit_transform(z_venue)\n",
    "z_auth_2d = umap.UMAP().fit_transform(z_auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(z_auth_2d[:,0],z_auth_2d[:,1],color=\"red\",alpha=0.5,label=\"author\")\n",
    "plt.scatter(z_venue_2d[:,0],z_venue_2d[:,1],color=\"blue\",alpha=0.5,label=\"venue\")\n",
    "plt.legend()\n",
    "plt.title(\"2D embedding\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
