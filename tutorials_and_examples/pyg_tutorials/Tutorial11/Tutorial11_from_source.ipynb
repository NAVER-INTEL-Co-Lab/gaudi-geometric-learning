{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 11: DeepWalk and node2vec - Implementation details\n",
    "  \n",
    "\n",
    "Paper:\n",
    "* [DeepWalk: Online Learning of Social Representation](https://arxiv.org/pdf/1403.6652.pdf)  \n",
    "* [node2vec: Scalable Feature Learning for Networks](https://arxiv.org/pdf/1607.00653.pdf)  \n",
    "\n",
    "Code:\n",
    "\n",
    " * [node2vec doc](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html?highlight=node2vec#torch_geometric.nn.models.Node2Vec)\n",
    " * [node2vec code](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/models/node2vec.html)\n",
    " * [Example on clustering](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/node2vec.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Use the eager mode\n",
    "os.environ['PT_HPU_LAZY_MODE'] = '0'\n",
    "\n",
    "# Verify the environment variable is set\n",
    "print(f\"PT_HPU_LAZY_MODE: {os.environ['PT_HPU_LAZY_MODE']}\")\n",
    "\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "import habana_frameworks.torch.core as htcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use rich traceback\n",
    "\n",
    "from rich import traceback\n",
    "traceback.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Node2Vec\n",
    "import os.path as osp\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'Cora'\n",
    "path = osp.join('.', 'data', dataset)\n",
    "dataset = Planetoid(path, dataset)\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ops.load_library(\"../../../raw_torch_for_scatter/random_walk/csrc/build/librandom_walk.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_hpu = torch.device(\"hpu\")\n",
    "device_cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_geometric/nn/models/node2vec.py\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_geometric.index import index2ptr\n",
    "from torch_geometric.typing import WITH_PYG_LIB, WITH_TORCH_CLUSTER\n",
    "from torch_geometric.utils import sort_edge_index\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "\n",
    "\n",
    "class Node2Vec(torch.nn.Module):\n",
    "    r\"\"\"The Node2Vec model from the\n",
    "    `\"node2vec: Scalable Feature Learning for Networks\"\n",
    "    <https://arxiv.org/abs/1607.00653>`_ paper where random walks of\n",
    "    length :obj:`walk_length` are sampled in a given graph, and node embeddings\n",
    "    are learned via negative sampling optimization.\n",
    "\n",
    "    .. note::\n",
    "\n",
    "        For an example of using Node2Vec, see `examples/node2vec.py\n",
    "        <https://github.com/pyg-team/pytorch_geometric/blob/master/examples/\n",
    "        node2vec.py>`_.\n",
    "\n",
    "    Args:\n",
    "        edge_index (torch.Tensor): The edge indices.\n",
    "        embedding_dim (int): The size of each embedding vector.\n",
    "        walk_length (int): The walk length.\n",
    "        context_size (int): The actual context size which is considered for\n",
    "            positive samples. This parameter increases the effective sampling\n",
    "            rate by reusing samples across different source nodes.\n",
    "        walks_per_node (int, optional): The number of walks to sample for each\n",
    "            node. (default: :obj:`1`)\n",
    "        p (float, optional): Likelihood of immediately revisiting a node in the\n",
    "            walk. (default: :obj:`1`)\n",
    "        q (float, optional): Control parameter to interpolate between\n",
    "            breadth-first strategy and depth-first strategy (default: :obj:`1`)\n",
    "        num_negative_samples (int, optional): The number of negative samples to\n",
    "            use for each positive sample. (default: :obj:`1`)\n",
    "        num_nodes (int, optional): The number of nodes. (default: :obj:`None`)\n",
    "        sparse (bool, optional): If set to :obj:`True`, gradients w.r.t. to the\n",
    "            weight matrix will be sparse. (default: :obj:`False`)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        edge_index: Tensor,\n",
    "        embedding_dim: int,\n",
    "        walk_length: int,\n",
    "        context_size: int,\n",
    "        walks_per_node: int = 1,\n",
    "        p: float = 1.0,\n",
    "        q: float = 1.0,\n",
    "        num_negative_samples: int = 1,\n",
    "        num_nodes: Optional[int] = None,\n",
    "        sparse: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.random_walk_fn = torch.ops.torch_cluster.random_walk\n",
    "\n",
    "        self.num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        row, col = sort_edge_index(edge_index, num_nodes=self.num_nodes).cpu()\n",
    "        self.rowptr, self.col = index2ptr(row, self.num_nodes), col\n",
    "\n",
    "        self.EPS = 1e-15\n",
    "        assert walk_length >= context_size\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.walk_length = walk_length - 1\n",
    "        self.context_size = context_size\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "\n",
    "        self.embedding = Embedding(self.num_nodes, embedding_dim,\n",
    "                                   sparse=sparse)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        self.embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, batch: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"Returns the embeddings for the nodes in :obj:`batch`.\"\"\"\n",
    "        emb = self.embedding.weight\n",
    "        return emb if batch is None else emb[batch]\n",
    "\n",
    "    def loader(self, **kwargs) -> DataLoader:\n",
    "        return DataLoader(range(self.num_nodes), collate_fn=self.sample,\n",
    "                          **kwargs)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def pos_sample(self, batch: Tensor) -> Tensor:\n",
    "        batch = batch.repeat(self.walks_per_node)\n",
    "        rw = self.random_walk_fn(self.rowptr, self.col, batch,\n",
    "                                 self.walk_length, self.p, self.q)        \n",
    "        if not isinstance(rw, Tensor):\n",
    "            rw = rw[0]\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def neg_sample(self, batch: Tensor) -> Tensor:\n",
    "        batch = batch.repeat(self.walks_per_node * self.num_negative_samples)\n",
    "\n",
    "        rw = torch.randint(self.num_nodes, (batch.size(0), self.walk_length),\n",
    "                           dtype=batch.dtype, device=batch.device)\n",
    "        rw = torch.cat([batch.view(-1, 1), rw], dim=-1)\n",
    "\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def sample(self, batch: Union[List[int], Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "        if not isinstance(batch, Tensor):\n",
    "            batch = torch.tensor(batch)\n",
    "        return self.pos_sample(batch), self.neg_sample(batch)\n",
    "\n",
    "    @torch.jit.export\n",
    "    def loss(self, pos_rw: Tensor, neg_rw: Tensor) -> Tensor:\n",
    "        r\"\"\"Computes the loss given positive and negative random walks.\"\"\"\n",
    "        # Positive loss.\n",
    "        start, rest = pos_rw[:, 0], pos_rw[:, 1:].contiguous()\n",
    "\n",
    "        h_start = self.embedding(start).view(pos_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(pos_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(out) + self.EPS).mean()\n",
    "\n",
    "        # Negative loss.\n",
    "        start, rest = neg_rw[:, 0], neg_rw[:, 1:].contiguous()\n",
    "\n",
    "        h_start = self.embedding(start).view(neg_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(neg_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(out) + self.EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def test(\n",
    "        self,\n",
    "        train_z: Tensor,\n",
    "        train_y: Tensor,\n",
    "        test_z: Tensor,\n",
    "        test_y: Tensor,\n",
    "        solver: str = 'lbfgs',\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> float:\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        clf = LogisticRegression(solver=solver, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        return clf.score(test_z.detach().cpu().numpy(),\n",
    "                         test_y.detach().cpu().numpy())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}({self.embedding.weight.size(0)}, '\n",
    "                f'{self.embedding.weight.size(1)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = device_cpu\n",
    "\n",
    "model = Node2Vec(data.edge_index, embedding_dim=128, walk_length=20,\n",
    "                 context_size=10, walks_per_node=10,\n",
    "                #  num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "                 num_negative_samples=1, p=1, q=1, sparse=False).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    z = model()\n",
    "    acc = model.test(z[data.train_mask], data.y[data.train_mask],\n",
    "                     z[data.test_mask], data.y[data.test_mask],\n",
    "                     max_iter=150)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "for epoch in trange(1, 201):\n",
    "    loss = train()\n",
    "    acc = test()\n",
    "    tqdm.write(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Acc: {acc:.4f}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_points(colors):\n",
    "    model.eval()\n",
    "    z = model(torch.arange(data.num_nodes, device=device))\n",
    "    z = TSNE(n_components=2).fit_transform(z.cpu().numpy())\n",
    "    y = data.y.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for i in range(dataset.num_classes):\n",
    "        plt.scatter(z[y == i, 0], z[y == i, 1], s=20, color=colors[i])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "colors = [\n",
    "    '#ffc0cb', '#bada55', '#008080', '#420420', '#7fe5f0', '#065535',\n",
    "    '#ffd700'\n",
    "]\n",
    "plot_points(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
